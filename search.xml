<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python 模拟 Ajax 请求抓取新浪微博]]></title>
    <url>%2FPython%E6%A8%A1%E6%8B%9FAjax%E8%AF%B7%E6%B1%82%E6%8A%93%E5%8F%96%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A.html</url>
    <content type="text"><![CDATA[Python 模拟Ajax 请求，抓取新浪微博。 Ajax是什么 全称为 Asynchronous JavaScript and XML，即异步的 JavaScript 和 XML，利用 JavaScript 在保证页面不被刷新、页面链接不变的情况下与服务器交换数据并更新部分网页的技术。 拿新浪微博为例，打开我的微博链接 https://m.weibo.cn/u/1715175570 ，一直下滑，可以发现下滑几个微博之后，会出现一个加载的动画，不一会儿就继续出现了新的微博内容，这个过程其实就是 Ajax 加载的过程。 Ajax 基本原理 这里不多做介绍，详细可参考 W3school 关于 Ajax 教程 分析请求 用Firefox浏览器打开我的微博链接 https://m.weibo.cn/u/1715175570 ，打开火狐浏览器自带的 Web 开发者工具，切换“网络”选项卡，可以发现这里出现很多条请求。Ajax 的请求类型是 xhr，可以通过工具栏上不同请求类型如 HTML、CSS、JS、XHR 等过滤请求。 点击”XHR“ 类型，过滤出 Ajax 请求再做分析。点击其中一条，可以查询该条请求的详细。在“消息头”选项卡中查看 请求网址，请求方法，以及请求头的详细，X-Requested-With 是 XMLHttpRequest，这就标记了此请求是 Ajax 请求。 随后可点击下 ”响应“，可以看到 JSON 格式的响应内容，返回的是个人信息，如昵称、简介等，这也是用来渲染个人主页所用的数据。 下滑页面以加载新的微博内容，可以看到，会有不断的 Ajax 请求发出。选择其中一个请求，分析它的参数信息。 这是一个 GET 请求，请求链接是 https://m.weibo.cn/api/container/getIndex?type=uid&amp;value=1715175570&amp;containerid=1076031715175570&amp;page=2 。 请求的参数有 4 个：type、value、containerid、page。 继续看接下来的请求，可以发现，type、value、containerid 始终没有变化，改变的值只有 page，很明显这个参数用来控制分页的，下图所示。 分析响应 观察这个请求的响应内容，所图所示： 响应内容是 JSON 格式的，data 数据分为两部分：cardlilstInfo、cards。cards 是一个列表，包含要提取的微博信息，比较重要的字段是 mblog。 包含的是微博一些信息，如 created_at（发布日期）、reposts_count（转发数目）、comments_count（评论数目）、attitudes_count（点赞数目）、text（微博正文）等。 这样可以请求一个接口，就可以获得一个 page 的微博，只需改变参数 page 即可。 实现代码 可以先定义一个获取单个 page 的函数 12345678910111213141516171819202122232425from urllib.parse import urlencodeimport requestsheaders = &#123; 'Host': 'm.weibo.cn', 'Referer': 'https://m.weibo.cn/u/1715175570', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0;) Gecko/20100101 Firefox/63.0', 'X-Requested-With': 'XMLHttpRequest' &#125;def get_page(page): """获取页面page微博列表""" params = &#123; 'type': 'uid', 'value': '1715175570', 'containerid': '1076031715175570', 'page': page &#125; url = base_url + urlencode(params) try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() except requests.ConnectionError as e: print('Error', e.args) 这里定义 base_url 来表示请求的 URL 前半部分，接下来，构造参数字典，调用 urlencode() 方法将参数转换成 URL 的 GET 请求参数。随后，base_url 与参数拼接成完整的 URL 。 用 requests 请求这个链接，加入 headers 参数，然后判断响应的状态码，若请求成功，返回 200，则调用 json() 方法将内容解析为 JSON 返回，否则不返回任何信息。 之后定义一个解析方法 parse_page(json)，用来从返回的 JSON 中提取信息，遍历 cards，获取 mblog 中的各个信息，赋值为一个信息的字典返回即可。 12345678910111213141516171819202122from pyquery import PyQuery as pqdef parse_page(json): """解析网页""" if json: items = json.get('data').get('cards') for item in items: item = item.get('mblog') weibo = &#123;&#125; # 发布日期 weibo['date'] = item.get('created_at') weibo['id'] = item.get('id') # 微博正文 weibo['text'] = pq(item.get('text')).text() weibo['source'] = item.get('source') # 转发数 weibo['reposts'] = item.get('reposts_count') # 评论数 weibo['comments'] = item.get('comments_count') # 点赞数 weibo['attitudes'] = item.get('attitudes_count') yield weibo 然后再定义一个方法将解析出来的结果储存到 MongoDB 中。 123456from pymongo import MongoClientdef save_to_mongo(result): """将返回结果result保存到MongoDB""" if collection.insert_many(result): print('Saved to mongodb') 另外定义一个获取长微博全文内容的方法，代码如下： 12345678910def longtext(id): """获取长微博内容""" url = 'https://m.weibo.cn/statuses/extend?id=' + id try: response = requests.get(url, headers=headers) if response.status_code == 200: longtext = response.json().get('data').get('longTextContent') return pq(longtext).text() except requests.ConnectionError as e: print('Error', e.args) 需要传入一个 id 参数，配合改写下解析方法parse_page(json)，整合代码，完整代码如下： 123456789101112131415161718192021222324from pyquery import PyQuery as pqimport reimport requestsdef parse_page(json): """解析网页""" if json: items = json.get('data').get('cards') for item in items: try: item = item.get('mblog') weibo = &#123;&#125; # 创建日期 weibo['date'] = item.get('created_at') weibo['id'] = item.get('id') # 判断是否为长微博 if item.get('isLongText') == True: content = pq(item.get('text')) result = re.search(r'&lt;a.*?status.*?(\d&#123;16&#125;).*?"', str(content)) lt_id = result.group(1) weibo['text'] = longtext(lt_id) else: # 微博正文 weibo['text'] = pq(item.get('text')).text() 修改下 get_page() 方法，将 value、containerid 同样做了参数化处理，这样可以方便爬取其他微博用户的信息。值得注意的是，返回的 JSON 内容在遍历 cards时可能会报错，原因在于微博有时会在 cards 列表中返回博主关注的信息，导致在解析时报 AttributeError，故在parse_page() 方法加上异常处理、以及抓取了转发微博的原微博内容。 最后在 main() 中通过 while 循环实现遍历所有页的微博内容。 运行结果： Studio 3T客户端展示结果如下： 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106'''获取微博'''from urllib.parse import urlencodefrom pyquery import PyQuery as pqfrom pymongo import MongoClientimport reimport timeimport randomimport requestsdef get_page(value, containerid, page): """获取页面微博列表""" params = &#123; 'type': 'uid', 'value': value, 'containerid': containerid, 'page': page &#125; url = base_url + urlencode(params) try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() except requests.ConnectionError as e: print('Error', e.args)def parse_page(json): """解析网页""" if json: items = json.get('data').get('cards') for item in items: try: item = item.get('mblog') weibo = &#123;&#125; # 创建日期 weibo['date'] = item.get('created_at') weibo['id'] = item.get('id') # 判断是否为长微博 if item.get('isLongText') == True: content = pq(item.get('text')) result = re.search(r'&lt;a.*?status.*?(\d&#123;16&#125;).*?"', str(content)) lt_id = result.group(1) weibo['text'] = longtext(lt_id) else: # 微博正文 weibo['text'] = pq(item.get('text')).text() weibo['source'] = item.get('source') # 转发数 weibo['reposts'] = item.get('reposts_count') # 评论数 weibo['comments'] = item.get('comments_count') # 点赞数 weibo['attitudes'] = item.get('attitudes_count') # 转发原文内容 if item.get('retweeted_status'): weibo['repost_text'] = pq(item.get('retweeted_status').get('text')).text() except AttributeError as e: continue yield weibodef longtext(id): """获取长微博内容""" url = 'https://m.weibo.cn/statuses/extend?id=' + id try: response = requests.get(url, headers=headers) if response.status_code == 200: longtext = response.json().get('data').get('longTextContent') # 通过pyquery方法去掉一些html标签 return pq(longtext).text() except requests.ConnectionError as e: print('Error', e.args)def save_to_mongo(result): """将返回结果result保存到MongoDB""" if collection.insert_many(result): print('Saved to mongodb')if __name__ == '__main__': value = '1862855661' containerid = '1076031862855661' base_url = 'https://m.weibo.cn/api/container/getIndex?' headers = &#123; 'Host': 'm.weibo.cn', 'Referer': 'https://m.weibo.cn/u/' + value, 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0;) Gecko/20100101 Firefox/63.0', 'X-Requested-With': 'XMLHttpRequest' &#125; myclient = MongoClient("mongodb://localhost:27017/") mydb = myclient["test"] collection = mydb["weibo" + value] page = 1 while True: print('*'*50) print('正在爬取：第%s 页' %page) json = get_page(value, containerid, page) if not json.get('ok') == 0: results = parse_page(json) save_to_mongo(results) page += 1 time.sleep(random.randint(1,4)) else: print("下载完最后一页!") break 本文完。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
        <tag>MongoDB</tag>
        <tag>Ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests库抓取猫眼电影排行]]></title>
    <url>%2Frequests%E5%BA%93%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C.html</url>
    <content type="text"><![CDATA[通过requests库，正则表达式抓取分析猫眼榜单100 并保存到MongoDB。 【摘要】：最近在学习爬虫，阅读崔大大的《Python 3网络爬虫开发实战》一书，获益匪浅。第一个爬虫实战就是抓取猫眼电影榜单100。 这里简单介绍下如何分析源代码，通过正则表达式爬取数据并保存到数据库。 准备工作确保已安装好 request 库，pymongo库，配置好 MongoDB 。 抓取分析抓取的目标站点是 https://maoyan.com/board/4 ，排名第一的电影是霸王别姬，页面中显示的有效信息有影片名称、主演、上映时间、上映地区、评分、海报等信息。 翻页到第二页，显示的结果是排行11~20 的电影，URL 变成 https://maoyan.com/board/4?offset=10 ，URL比之前多了一个offset 的参数，继续翻页，显示的结果是排行21~30 的电影，URL 变为 https://maoyan.com/board/4?offset=20 总结规律：URL 中参数 offset 代表偏移量值，如果偏移量为n ，则显示的电影序号就是n+1 到n+10, 每页显示10 个。获取 TOP100 电影，则分开请求10 次即可。 抓取单页面首先抓取单个页面信息，定义一个 get_one_page()方法，具体代码如下 1234567891011121314151617181920212223242526272829'''抓取猫眼电影榜单'''import reimport requestsfrom fake_useragent import UserAgentdef get_one_page(url): """获取url的电影信息""" try: ua = UserAgent() headers = &#123; # 引用fake_useragent随机生成一个User-Agent 'User-Agent': ua.random &#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text return None except RequestException: return None def main(): """ """ url = 'https://maoyan.com/board/4' html = get_one_page(url) print(html)main() 正则提取电影信息要获取页面影片名称、主演、上映时间、上映地区、评分、海报等信息，分析页面源码，利用Web 开发者工具，查看“网络”-“响应”，获取到response 源代码。 分析其中一个条目，可以看到，一部电影信息对应的源代码是一个dd 节点，我们用正则表达式来提取这里面的一些电影信息。首先，需要提取它的排名信息。排名位于 class 为 board-index 的 i 节点内，正则表达式写为： 1&apos;&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt; 随后提取电影的海报信息，后面有a 节点，其内部有两个img 节点。分别尝试打开两个 img 链接，第二个 img 节点data-src 属性是海报的链接。 提取第二个img 节点的data-src 属性，正则表达式改写为： 1&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot; 再往后，依次提取电影的名称、主演、发布时间、评分等内容，最终的正则表达式写为： 1&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt; 接下来通过调用 findall方法提取出所有的内容。 实现代码如下： 1234567891011121314def parse_one_page(html): """解析单页电影信息""" pattern = re.compile(r'&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src="(.*?)".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html) # 遍历提取结果，去掉提取结果中不必要的信息（主演、上映时间）并生成字典 for item in items: yield &#123; 'index': item[0], 'image': item[1], 'title': item[2].strip(), 'actor': item[3].strip()[3:] if len(item[3]) &gt; 3 else '', 'time': item[4].strip()[5:] if len(item[4]) &gt; 5 else '', 'score': item[5].strip() + item[6].strip() &#125; 写入MongoDB数据实现代码如下： 12345678def write_to_mongodb(data): """写入MongoDB数据库""" # 创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。 myclient = pymongo.MongoClient("mongodb://localhost:27017/") # 创建数据库test和集合collections，注意使用[] mydb = myclient["test"] mycol = mydb["top100"] mycol.insert_one(data) 代码整合实现main()方法来调用前面实现的方法，将单页的电影结果写入到数据库。 123456def main(): """main()""" url = 'https://maoyan.com/board/4' html = get_one_page(url) for item in parse_one_page(html): write_to_mongodb(item) 分页爬取因为我们需要抓取的是TOP100 的电影，所以还需要遍历一下，给这个链接传入offset 参数，实现其他90 部电影的爬取，此时添加如下调用即可： 123if __name__ == '__main__': for i in range(10): main(offset=i*10) 对应的修改下main()函数，接收一个offset 值作为偏移量，然后构造URL 进行爬取。 123456def main(offset): """main()""" url = 'https://maoyan.com/board/4?offset='+ str(offset) html = get_one_page(url) for item in parse_one_page(html): write_to_mongodb(item) 运行结果电影榜单TOP100 成功保存到 MongoDB 数据库中。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263'''抓取猫眼电影排行'''import reimport timeimport pymongoimport requestsfrom requests.exceptions import RequestExceptionfrom fake_useragent import UserAgentdef get_one_page(url): """获取url的电影信息""" try: ua = UserAgent() headers = &#123; # 引用fake_useragent随机生成一个User-Agent 'User-Agent': ua.random &#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text return None except RequestException: return Nonedef parse_one_page(html): """解析单页电影信息""" pattern = re.compile(r'&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src="(.*?)".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html) for item in items: yield &#123; 'index': item[0], 'image': item[1], 'title': item[2].strip(), 'actor': item[3].strip()[3:] if len(item[3]) &gt; 3 else '', 'time': item[4].strip()[5:] if len(item[4]) &gt; 5 else '', 'score': item[5].strip() + item[6].strip() &#125;def write_to_mongodb(data): """写入MongoDB数据库""" # 创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。 myclient = pymongo.MongoClient("mongodb://localhost:27017/") # 创建数据库test和集合collections，注意使用[] mydb = myclient["test"] mycol = mydb["top100"] mycol.insert_one(data)def main(offset): """main()""" url = 'https://maoyan.com/board/4?offset='+ str(offset) html = get_one_page(url) for item in parse_one_page(html): write_to_mongodb(item)if __name__ == '__main__': for i in range(10): main(offset=i*10) # 每请求一次，增加了一个延时等待，防止请求过快 time.sleep(1.5) 参考链接：《Python 3网络爬虫开发实战 ,崔庆才著》 Python Mongodb]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tesserocr解析库]]></title>
    <url>%2Ftesserocr%E8%A7%A3%E6%9E%90%E5%BA%93.html</url>
    <content type="text"><![CDATA[OCR，即Optical Character Recognition，光学字符识别。 是指通过扫描字符，然后通过其形状将其翻译成电子文本的过程。对于图形验证码来说，它们都是一些不规则的字符，这些字符确实是由字符稍加扭曲变换得到的内容。 tesserocr 是 Python 的一个OCR 识别库，但其实是对 tesseract 做的一层Python API 封装，所以它的核心是 tesseract 。在安装 tesserocr 之前，需要先安装 tesseract。 链接 tesserocr GitHub：https://github.com/sirfz/tesserocr tesserocr PyPI：https://pypi.python.org/pypi/tesserocr tesseract 下载地址：https://digi.bib.uni-mannheim.de/tesseract/ tesseract GitHub：https://github.com/tesseract-ocr/tessdata tesseract 语言包：https://github.com/tesseract-ocr/tessdata tesseract下载地址 https://digi.bib.uni-mannheim.de/tesseract/ 带有 dev 的为开发版本 带alpha 的为内部测试版本 带beta 的为公开测试版 带 rc 的为Release Candidate（候选版本） 其他为稳定版本，推荐选择 tesseract-3.05 的稳定版本。 下载完成后双击，可以勾选 Additional language data ( download）选项来安装OCR 识别支持的语言包，这样OCR便可以识别多国语言，然后一路默认，点击 Next 按钮即可。 不建议勾选Additional language data ( download）选项，因为速度比较慢，可以安装后直接下载语言包，然后将语言包复制到安装目录的 tessdata 目录下即可。 tesserocr使用命令： pip install tesserocr pillow 若安装报错，提示信息如下： 1error: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft Visual C++ Build Tools&quot;: https://visualstudio.microsoft.com/downloads/ 解决方法： 下载对应版本的whl 包（和下载的tesseract 版本对应）， 地址：https://github.com/simonflueckiger/tesserocr-windows_build/releases ， 然后使用命令行安装： pip install &lt;package_name&gt;.whl 验证安装以如下面所示的图片为样例进行测试。 首先使用命令行进行测试，使用 tesseract 命令： 1234C:\Users\Keymou\Desktopλ tesseract image.png result -l eng &amp;&amp; cat result.txtTesseract Open Source OCR Engine v3.05.02 with LeptonicaPython3WebSpider 其中第一个参数为图片名称，第二个参数result 为结果保存的目标文件名称，-l 指定使用的语言包，在此使用英文（ eng ）。然后，再用cat 命令将结果输出。 运行结果便是图片的识别结果：Python3WebSpider。 tesseract imagename|stdin outputbase|stdout [options...] [configfile...] 用Python 代码来测试，借助于 tesserocr 库。 测试代码 1234import tesserocrfrom PIL import Imageimage = Image.open(r&apos;D:\\python\\image.png&apos;)print(tesserocr.image_to_text(image)) 运行结果如下： Python3WebSpider 或者 直接在 cmd 中 直接调用 file_to_text()方法，代码过程如下： 123456λ pythonPython 3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)] :: Anaconda, Inc. on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import tesserocr&gt;&gt;&gt; print(tesserocr.file_to_text(&apos;image.png&apos;))Python3WebSpider 推荐一个颜值很高的 cmd 工具，Cmder。 如果成功输出结果，则证明 tesseract 和 tesserocr 都已经安装成功。 FAQ安装 tesserocr 折腾了很久，安装过程或许会碰到意外情况，不要慌，首先上网搜索看看，是否能解决，根据实际情况排查解决问题。 常见报错信息： RuntimeError: Failed to init API, possibly an invalid tessdata path: D:\ProgramData\Anaconda3\ 分析错误信息，是初始化API 失败，可能是一个无效的tessdata 路径。检查下后面给出path 下是否有 tessdata 目录。 第一种情况，没有该目录，则需要新建目录，然后将 tesseract 安装目录下的tessdata 复制该path 下。若提示的path 存在并且已经有tessdata 目录，就需要检查下是否环境变量没有配置。新增环境变量 TESSDATA_PREFIX，变量值为 指向 tessdata 的路径，如 C:\Tesseract-OCR\tessdata 第二种情况，path目录下存在 tessdata，环境变量也已生效，但python 代码测试时仍报 API 初始化错误，可能需要考虑是否版本问题。我折腾了很久，安装的 tesseract 4.0，但一直报错，换成tesseract-3.05，问题解决。 python3 通过Anaconda3 安装的，可以通过以下命令试安装： &gt; conda install -c simonflueckiger tesserocr]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
        <tag>tesserocr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发生命周期]]></title>
    <url>%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F.html</url>
    <content type="text"><![CDATA[计算机行业流行一个笑话：有三样东西在制造过程中是永远看不见的——法律、香肠和软件。 这种说法不完全对，但有些软件开发严格有序，有些软件控制却混乱不堪。软件产品从最初构思到公开发行的过程称为软件开发生命周期模式。 以下是常用的模式： 大爆炸模式 最简单的软件开发模式，优点是简单。计划、进度安排和正规开发过程几乎没有，所有精力花费在开发软件和编写代码上。 编写边改模式 通常只有粗略的想法，进行一些简单的设计，然后开始漫长的来回编写、测试和修改缺陷的过程，反复直到觉得足够了，就发布产品。 瀑布模式 最典型的预见性的方法，严格遵循预先计划的需求分析、设计、编码、集成、测试、维护的步骤顺序进行。步骤成果作为衡量进度的方法，例如需求规格，设计文档，测试计划和代码审阅等等。 瀑布式的主要的问题是它的严格分级导致的自由度降低，项目早期即作出承诺导致对后期需求的变化难以调整，代价高昂。瀑布式方法在需求不明并且在项目进行过程中可能变化的情况下基本是不可行的。 螺旋模式 核心就在于不需要在刚开始的时候就把所有事情都定义的清清楚楚。轻松上阵，定义最重要的功能，实现它，然后听取客户的意见，之后再进入到下一个阶段。 螺旋模式每一次循环包括6个步骤： 1）明确目标、可选方案和限制条件。 2）明确并化解风险。 3）评估可选方案。 4）当前阶段的开发和测试。 5）计划下一阶段。 6）确定进入下一阶段的方法。 螺旋模型很大程度上是一种风险驱动的方法体系，因为在每个阶段之前及经常发生的循环之前，都必须首先进行风险评估。 迭代式开发 也被称作迭代增量式开发或迭代进化式开发，是一种与传统的瀑布式开发相反的软件开发过程，它弥补了传统开发方式中的一些弱点，具有更高的成功率和生产率。 每次只设计和实现这个产品的一部分，逐步逐步完成的方法叫迭代开发，每次设计和实现一个阶段叫做一个迭代。 在迭代开发方法中，每一次迭代都包括了需求分析、设计、实现与测试。采用这种方法，开发工作可以在需求被完整地确定之前启动，并在一次迭代中完成系统的一部分功能或业务逻辑的开发工作。再通过客户的反馈来细化需求，并开始新一轮的迭代。 迭代式开发的优点： 1、降低风险 2、得到早期用户反馈 3、持续的测试和集成 4、使用变更 5、提高复用性 敏捷软件开发 又称敏捷开发，是一种应对快速变化的需求的一种软件开发能力。更强调程序员团队与业务专家之间的紧密协作、面对面的沟通、频繁交付新的软件版本、紧凑而自我组织型的团队、能够很好地适应需求变化的代码编写和团队组织方法，也更注重软件开发中人的作用。 敏捷开发的目的： 通过过程和工具理解个人和交流的作用 通过全面的文档理解运行的软件 通过合同和谈判得到客户的协作 在计划的执行中做出对变更的响应 迭代开发是一种软件开发的生命周期模型，敏捷开发是多种软件开发项目管理方法的集合，其中包括了XP、Scrum等，简单来说，迭代式开发模型是敏捷开发普遍使用的软件生命周期模型，敏捷开发所包含的内容比迭代模型宽泛的多。]]></content>
      <categories>
        <category>软件开发</category>
      </categories>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[アンナチュラル]]></title>
    <url>%2F%E3%82%A2%E3%83%B3%E3%83%8A%E3%83%81%E3%83%A5%E3%83%A9%E3%83%AB.html</url>
    <content type="text"><![CDATA[又是一个周末啦，安利一部日剧《アンナチュラル》非自然死亡 简介三澄美琴（石原里美 饰）是在民间法医组织“UDI”工作的女法医，该组织专门接收由于非正常原因导致死亡的遗体，对其进行解剖以求找到案件的真相。和美琴一起工作的，还有法医中堂系（井浦新 饰）、记录员九部六郎（洼田正孝 饰）和检查技师东海林夕子（市川实日子 饰）等人。 中堂系虽然拥有着丰富的临床经验，个性却乖僻古怪，对正义和法律理解不同的美琴和中堂之间，常常产生无法调和的矛盾。其实，中堂有一个无人知晓的秘密，他的女友在一场“意外”中不幸丧生，可种种蛛丝马迹向中堂揭示了，是一名连环杀人犯取走了女友的性命。中堂不畏人言坚定的留在UDI，正是为了找到杀死女友的凶手。 剧情我就不剧透了，优酷、芒果有资源。 优酷 | 芒果TV 经典台词伴侣就要找那种睡相让你觉得很喜欢的人男女关系中是不会只有一方有错的有工夫绝望的话 还不如吃点好吃的去睡觉呢只是把孩子当作自己的所有物，不明白孩子跟自己是互相独立的个体对女性的歧视人这种生物 不管是谁切开来剥皮后都只是一团肉而已死了就明白了 特别认同中堂这段话，人生不就是这样嘛 为了活下去 梦想什么的也没必要说得那么夸张有个目标就行 每个人都是罪人 为了赎罪而工作 不管女性穿什么样的衣服或者喝得酩酊大醉都不能成为肆意妄为的理由没有得到双方一致同意的性行为就是犯罪 欺凌杀人 你就算献出了自己的生命你的痛楚肯定也无法传达给他们你的人生 &nbsp;属于你自己 看官点个赞再走吧]]></content>
      <categories>
        <category>日剧</category>
      </categories>
      <tags>
        <tag>石原里美</tag>
        <tag>日剧</tag>
        <tag>Unnatural</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常见知识点]]></title>
    <url>%2Fpython%E5%B8%B8%E8%A7%81%E7%9F%A5%E8%AF%86%E7%82%B9.html</url>
    <content type="text"><![CDATA[最近在学习python，总结了下python常用的知识点。 函数参数传递1234567891011a = 1def fun(a): a = 2fun(a)print(a)b = []def fun(b): b.append(1)fun(b)print(b) 所有的变量都可以理解是内存中一个对象的“引用”。类型是属于对象的，而不是变量。而对象有两种,“可更改”（mutable）与“不可更改”（immutable）对象。在python中，strings, tuples, 和numbers是不可更改的对象，而list,dict等则是可以修改的对象。 实例方法、类方法、静态方法12345678910111213141516171819def foo(x): print("executing foo(%s)"%(x))class A(object): def foo(self,x): print("executing foo(%s,%s)"%(self,x)) @classmethod def class_foo(cls,x): print("executing class_foo(%s,%s)"%(cls,x)) @staticmethod def static_foo(x): print("executing static_foo(%s)"%x)a=A()a.foo(2)a.class_foo(2)a.static_foo(2) 先理解下函数参数里面的self和cls。这个self和cls是对类或者实例的绑定,对于一般的函数来说我们可以这么调用foo(x),这个函数就是最常用的,它的工作跟任何东西(类,实例)无关.对于实例方法,我们知道在类里每次定义方法的时候都需要绑定这个实例,就是foo(self, x),为什么要这么做呢?因为实例方法的调用离不开实例,我们需要把实例自己传给函数,调用的时候是这样的a.foo(x)(其实是foo(a, x)).类方法一样,只不过它传递的是类而不是实例,A.class_foo(x).注意这里的self和cls可以替换别的参数,但是python的约定是这俩,还是不要改的好. 对于静态方法其实和普通的方法一样,不需要对谁进行绑定,唯一的区别是调用的时候需要使用a.static_foo(x)或者A.static_foo(x)来调用. \ 实例方法 类方法 静态方法 a = A() a.foo(x) a.class_foo(x) a.static_foo(x) A 不可用 A.class_foo(x) A.static_foo(x) 类变量、实例变量123456789class Person: name="aaa" p1=Person()p2=Person()p1.name="bbb"print(p1.name) # bbbprint(p2.name) # aaaprint(Person.name) # aaa 类变量就是供类使用的变量,实例变量就是供实例使用的。 Python自省123a = 1b = 'Hello'print(type(a), type(b)) 自省就是面向对象的语言所写的程序在运行时,所能知道对象的类型.简单一句就是运行时能够获得对象的类型。比如type()、dir()、getattr()、hasattr()、isinstance() 列表推导式123456789101112multiples = [i for i in range(30) if i % 3 is 0]print(multiples)mcase = &#123;'a': 10, 'b': 34, 'A': 7, 'Z': 3&#125;mcase_frequency = &#123; k.lower(): mcase.get(k.lower(), 0) + mcase.get(k.upper(), 0) for k in mcase.keys()&#125;print(mcase_frequency)# mcase_frequency == &#123;'a': 17, 'z': 3, 'b': 34&#125; 列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表。规范:variable = [out_exp for out_exp in input_list if out_exp == 2] 字典推导式，上述例子把同一个字母但不同大小写的值合并起来。可以快速对换一个字典的键和值：{v: k for k, v in dict.items()} 单下划线和双下划线12345678910class MyClass(): def __init__(self): self.__superprivate = 'Hello' self._semiprivate = ', world'mc = MyClass()print(mc._semiprivate)print(mc.__dict__)# print(mc.__superprivate)# AttributeError: 'MyClass' object has no attribute '__superprivate' __foo__：一种约定,Python内部的名字,用来区别其他用户自定义的命名,以防冲突. _foo：一种约定,用来指定变量私有.程序员用来指定私有变量的一种方式. __foo：这个有真正的意义:解析器用_classname__foo来代替这个名字,以区别和其他类相同的命名. 字符串格式化：%和.format的区别1234567name = 'Joe'print('Name is %s' %name)print('Name is &#123;&#125;'.format(name))name = (1, 2, 3)# print('Name is %s' %name) # TypeError: not all arguments converted during string formattingprint('Name is &#123;&#125;'.format(name))print('Name is %s' %(name,)) format简洁，%无法同时传递一个变量和元组 迭代器和生成器123456789101112131415161718192021222324252627mylist = [1, 2, 3]for i in mylist: print(i)# mylist is an iterable# These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.# yield is a keyword that is used like return, except the function will return a generator.def createGenerator(): mylist = range(3) for i in mylist: yield i*i# create a generatormygenerator = createGenerator()print(mygenerator)for i in mygenerator: print(i)print('****')# To master yield, you must understand that when you call the function, the code you have written in the function body does not run.for j in mygenerator: print(j)"""Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly.""" *args and **kwargs”不确定你的函数里将要传递多少参数时你可以用*args。例如，它可以传递任意数量的参数12345def print_everything(*args): for count, thing in enumerate(args): print('&#123;0&#125;. &#123;1&#125;'.format(count, thing))print_everything('apple', 'banana', 'cabbage') **kwargs允许你使用没有事先定义的参数名 12345def table_things(**kwargs): for name, value in kwargs.items(): print('&#123;0&#125;=&#123;1&#125;'.format(name, value))table_things(apple = 'fruit', cabbage = 'vegetable') 也可以混着用.命名参数首先获得参数值然后所有的其他参数都传递给*args和**kwargs。命名参数在列表的最前端，*args和**kwargs可以同时在函数的定义中，但是*args必须在**kwargs前面. 面向切面编程AOP和装饰器 装饰器是一个很著名的设计模式，经常被用于有切面需求的场景，较为经典的有插入日志、性能测试、事务处理等。 装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 123456789def makebold(fn): def wrapped(): return "&lt;b&gt;" + fn() + "&lt;/b&gt;" return wrappeddef makeitalic(fn): def wrapped(): return "&lt;i&gt;" + fn() + "&lt;/i&gt;" return wrapped 装饰器的作用就是为已经存在的对象添加额外的功能 123456@makebold@makeitalicdef hello(): return "hello world"print(hello()) 鸭子类型当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。并不关心对象是什么类型，到底是不是鸭子，只关心行为。在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。 Python中函数重载 函数重载主要是为了解决两个问题:可变参数类型、可变参数个数。 一个基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载。情况1：函数功能相同，但是参数类型不同，根本不需要处理，因为python可以接受任何类型的参数情况2：函数功能相同，但参数个数不同，答案就是缺省参数。python 自然就不需要函数重载。 new和init的区别 __new__是一个静态方法,而__init__是一个实例方法. __new__方法会返回一个创建的实例,而__init__什么都不返回. 只有在__new__返回一个cls的实例时后面的__init__才能被调用. 当创建一个新实例时调用__new__,初始化一个实例时用__init__. ps: metaclass是创建类时起作用.所以我们可以分别使用metaclass,new和init来分别在类创建,实例创建和实例初始化的时候做一些小手脚. 单例模式使用new方法123456789class Singleton(object): def __new__(cls, *args, **kw): if not hasattr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls, *args, **kw) return cls._instance class MyClass(Singleton): a = 1 共享属性 创建实例时把所有实例的__dict__指向同一个字典,这样它们具有相同的属性和方法.123456789class Borg(object): _state = &#123;&#125; def __new__(cls, *args, **kw): ob = super(Borg, cls).__new__(cls, *args, **kw) ob.__dict__ = cls._state return ob class MyClass2(Borg): a = 1 装饰器版本1234567891011def singleton(cls, *args, **kw): instances = &#123;&#125; def getinstance(): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return getinstance @singletonclass MyClass: ... import方法123456# mysingleton.pyclass My_Singleton(object): def foo(self): pass my_singleton = My_Singleton() 123# to use# from mysingleton import my_singleton# my_singleton.foo() 作用域 一个变量的作用域总是由在代码中被赋值的地方所决定的。 当 Python 遇到一个变量的话他会按照这样的顺序进行搜索： 本地作用域（Local）→当前作用域被嵌入的本地作用域（Enclosing locals）→全局/模块作用域（Global）→内置作用域（Built-in） 闭包(closure)闭包(closure)是函数式编程的重要的语法结构创建一个闭包必须满足以下几点: 必须有一个内嵌函数 内嵌函数必须引用外部函数中的变量 外部函数的返回值必须是内嵌函数 lambda函数lambda 表达式，通常是在需要一个函数，但是又不想费神去命名一个函数的场合下使用，也就是指匿名函数。1map( lambda x: x*x, [y for y in range(10)] ) Python里的拷贝 copy浅拷贝，没有拷贝子对象，所以原始数据改变，子对象会改变 深拷贝，包含对象里面的自对象的拷贝，所以原始对象的改变不会造成深拷贝里任何子元素的改变1234567891011121314151617181920import copya = [1, 2, 3, 4, ['a', 'b']] #原始对象 b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝 a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象 print('a = ', a)print('b = ', b)print('c = ', c)print('d = ', d)# 输出结果：# a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# c = [1, 2, 3, 4, ['a', 'b', 'c']]# d = [1, 2, 3, 4, ['a', 'b']] Python垃圾回收机制Python GC主要使用引用计数（reference counting）来跟踪和回收垃圾。在引用计数的基础上，通过“标记-清除”（mark and sweep）解决容器对象可能产生的循环引用问题，通过“分代回收”（generation collection）以空间换时间的方法提高垃圾回收效率。 引用计数PyObject是每个对象必有的内容，其中ob_refcnt就是做为引用计数。当一个对象有新的引用时，它的ob_refcnt就会增加，当引用它的对象被删除，它的ob_refcnt就会减少.引用计数为0时，该对象生命就结束了。 标记-清除机制基本思路是先按需分配，等到没有空闲内存的时候从寄存器和程序栈上的引用出发，遍历以对象为节点、以引用为边构成的图，把所有可以访问到的对象打上标记，然后清扫一遍内存空间，把所有没标记的对象释放。 分代技术整体思想是：将系统中的所有内存块根据其存活时间划分为不同的集合，每个集合就成为一个“代”，垃圾收集频率随着“代”的存活时间的增大而减小，存活时间通常利用经过几次垃圾回收来度量。 Python的isis是对比地址，==是对比值 read，readline和readlines read 读取整个文件 readline 读取下一行,使用生成器方法 readlines 读取整个文件到一个迭代器以供我们遍历]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
