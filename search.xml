<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Selenium 3 自动化测试实践]]></title>
    <url>%2FSelenium%203%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E5%AE%9E%E8%B7%B5.html</url>
    <content type="text"><![CDATA[介绍下Web 应用程序界面常用的自动化测试框架 Selenium。 SeleniumSelenium 是什么Selenium 是用于测试 Web 应用程序用户界面（UI）的自动化测试框架。它是一款用于运行端到端功能测试的超强工具。您可以使用多个编程语言编写测试，并且 Selenium能够在一个或多个浏览器中执行这些测试。 特点： 开源，免费； 多浏览器支持：Firefox、Chrome、IE、Edge、Opera； 多平台支持：Linux、Windows、MAC； 多语言支持：Java、Python、Ruby、C#、JavaScript； API 简单、灵活 通过 Selenium，可以编写代码让浏览器： 自动加载网页，获取当前呈现页面的源码； 模拟点击和其他交互方式，最常用：模拟表单提交(比如模拟登录)； 截取页面； 判断网页某些动作是否发生，如页面是否刷新，等等。 历史版本Selenium 1.02004年，ThoughtWorks公司的 JasonHuggins 和他所在的团队采用 Javascript 编写一种测试工具来验证浏览器页面的行为。这个JavaScript 类库就是 Selenium core，同时也是 seleniumRC、Selenium IDE 的核心组件。 之后 Paul Hammant 加入团队并指导开发第二种操作模式，后来成为 Selenium Remote Control（RC）。 关于命名 当时QTP mercury是主流的商业自动化工具，是化学元素汞（俗称水银），而Selenium是开源自动化工具，是化学元素硒，硒可以对抗汞。 Selenium1.0 框架组成： Selenium 1.0 = Selenium IDE + Selenium Grid + SeleniumRC Selenium IDE： Selenium IDE 是嵌入到Firefox浏览器中的一个插件，实现简单的浏览器操作的录制与回放功能； 官方给出它自身作用的定位： 快速地创建bug重现脚本，在测试人员测试过程中，发现bug之后可以通过IDE将重现的步骤录制下来，以帮助开发人员更容易的重现bug； 可访问这里下载对应 IDE。由于实践中不涉及用Selenium IDE 生成脚本，这里不多做介绍了。 Selenium Grid： Selenium Grid 是一种自动化的测试辅助工具，Grid 通过利用现有的计算机基础设施，能加快Web-App 的功能测试。利用Grid 可以很方便地实现在多台机器上和异构环境中运行测试用例。 工作原理： Selenium Grid 实际它是基于Selenium RC 的，而所谓的分布式结构就是由一个hub 节点和若干个node 代理节点组成。Hub 用来管理各个代理节点的注册信息和状态信息，并且接受远程客户端代码的请求调用，然后把请求的命令转发给代理节点来执行。 Selenium RC： Selenium RC（Remote Control）是Selenium家族的核心部分。Selenium RC 支持多种不同语言编写的自动化测试脚本，通过Selenium RC的服务器作为代理服务器去访问应用，从而达到测试的目的。 Selenium RC 分为Client Libraries 和Selenium Server。Client Libraries 库主要用于编写测试脚本，用来控制Selenium Server 的库。Selenium Server 负责控制浏览器行为。 Selenium 2.02006年，Google 的工程师SimonStewart 发起了WebDriver 的项目，因为长期以来Google 一直是Selenium 的重度用户，但却被限制在有限的操作范围内。 Selenium RC 与WebDriver 区别： Selenium RC 是在浏览器中运行JavaScript 应用，使用浏览器内置的JavaScript 翻译器来翻译和执行selenses 命令（selenses 是Selenium 命令集合）。 WebDriver 是通过原生浏览器支持或浏览器扩展来直接控制浏览器。 WebDriver 针对各个浏览器而开发，取代了嵌入到被测Web 应用中的JavaScript，与浏览器紧密集成，因此支持创建更高级的测试，避免了JavaScript 安全模型导致的限制。除了来自浏览器厂商的支持之外，WebDriver 还利用操作系统级的调用，模拟用户输入。 Selenium 与WebDriver 原是属于两个不同的项目，WebDriver 的创建者Simon Stewart 早在2009年8月的一份邮件中解释了项目合并的原因。 Selenium 与WebDriver 合并原因：为何把两个项目合并？部分原因是WebDriver 解决了Selenium 存在的缺点（例如能够绕过JavaScript 沙箱，我们有出色的API ），部分原因是Selenium 解决了WebDriver 存在的问题（例如支持广泛的浏览器），部分原因是因为Selenium 的主要贡献者和我都觉得合并项目是为用户提供最优先框架的最佳途径。 Selenium 2.0 = Selenium 1.0 + WebDriver 需要强调的是，在Selenium 2.0 中主推的是 WebDriver，可以将其看作SeleniumRC 的替代品。因为Selenium 为了保持向下的兼容性，所以在Selenium 2.0 中并没有彻底地抛弃Selenium RC。 Selenium 2具有来自WebDriver 的清晰面向对象 API ，并能以最佳的方式与浏览器进行交互。Selenium 2不使用 JavaScript 沙盒，它支持多种浏览器和多语言绑定。 Selenium 2为下列程序提供驱动程序： Mozilla Firefox Google Chrome Microsoft Internet Explorer Opera Apple iPhone Android browsers …… 借助 Selenium 2，您可使用 Java、C#、Ruby、和 Python 编写测试。Selenium 2 还提供基于 HtmlUnit的无外设驱动，是用于测试 Web应用程序的 Java框架。HtmlUnit运行速度特别快，但它不是一个真正与真实浏览器相关联的驱动。 WebDriver 原理： WebDriver 是按照Server-Client 的设计模式设计的； Server 端就是Remote Server，可以是任意的浏览器；当我们的脚本启动浏览器后，该浏览器是Remote Server，职责是等待Client 发送请求并做出响应。 Client 端简单理解是测试代码；脚本的行为是以http 请求的方式发送给测试的浏览器，浏览器接受请求，执行相应操作，并在response 中返回执行状态，返回等信息。 WebDriver 的工作流程： WebDriver 启动目标浏览器，并绑定到指定端口；启动的浏览器实例将作为WebDriver 的Remote Server； Client 端通过CommandExcuter 发送HTTPRequest 给Remote Server 的侦听端口； Remote Server 需要依赖原生的浏览器组件来转化浏览器的native调用； Selenium 3.02016年7月，Selenium3.0 发布了第一个Beta 版本。 Selenium 3.0 = Selenium 2.0 + Selenium RC（Remote Control） Selenium 3 核心的安装包中彻底删除了Selenium RC 。 安装直接使用 pip 命令安装，命令如下： 1pip install selenium 查看安装 selenium 是否成功，显示 selenium 版本信息则安装成功。 1pip show selenium 接下来还需要下载浏览器驱动，Selenium 是不支持浏览器功能的，需要和第三方的浏览器一起搭配使用，支持下述浏览器，你需要把对应的浏览器驱动下载到 Python 目录下： Chrome Firefox PhantomJS IE Edge Opera Safari 不过得注意一点，浏览器驱动版本支持的浏览器版本，拿 Chrome driver 与 chrome版本对应关系举例： Chrome driver 版本 Chrome 版本 v2.45 v70-72 v2.44 v69-71 v2.43 v69-71 v2.42 v68-70 v2.41 v67-69 v2.40 v66-68 v2.39 v66-68 v2.38 v65-67 v2.37 v64-66 v2.36 v63-65 通过在 Chrome 浏览器地址栏输入以下命令，获取浏览器版本信息： 1chrome://version/ 主要是关注版本号：71.0.3578.98 (正式版本)，参考上面选择下载对应 Chrome v71版本的 Chrome driver，选择下载 v2.45，根据操作系统选择对应的浏览器驱动，Windows 平台 chrome driver 没有 64位版本，选择下载 win32版本即可。 下载完成后，解压 zip文件，解压后的 chromedriver.exe拷贝到 Python的 \Scripts目录下。Mac 的话把解压后的文件拷贝到 usr/local/bin目录下，Linux 环境则拷贝到 usr/bin目录下。 初步体验先来一个例子，感受一下 Selenium： 1234567891011121314#simple.py# 导入selenium 的 webdriver包，只有导入 webdriver 包我们才能使用webdriver API 进行自动化脚本的开发。from selenium import webdriver# 创建Firefox WebDriver的实例driver = webdriver.Firefox()# driver.get()方法将导航到URL指向的页面driver.get("http://www.python.org")# 获取当前页面的源并打印，Gets the source of the current page.html_text = driver.page_sourceprint(html_text)# 关闭浏览器窗口driver.close() 执行代码，自动调用 Firefox浏览器，并访问 http://www.python.org ，控制台会输出当前页面的代码。 WebDriver API导航想要用 WebDriver做的第一件事就是导航到一个链接。通常的方法是调用get方法： 12345678browser = webdriver.Firefox()# 声明浏览器对象# browser = webdriver.Chrome()# browser = webdriver.Edge()# browser = webdriver.PhantomJS()# browser = webdriver.Safari()browser.get("http://www.google.com") 控制窗口大小12345678910111213# 设置当前窗口的x，y位置driver.set_window_position(110, 120)# 设置当前窗口的宽度和高度driver.set_window_size(width=800, height=900)# 设置窗口的x，y坐标以及当前窗口的高度和宽度driver.set_window_rect(x=10, y=10, width=400, height=300)# driver.set_window_rect(width=400, height=300)# driver.set_window_rect(x=10, y=10)# 当前窗口最大化driver.maximize_window() 页面标题和链接12345# 获取当前网页的标题driver.title# 获取当前网页的urldriver.current_url 页面源12345# 获取当前页面的源，返回的是str类型driver.page_source# 返回当前上下文（Native或WebView），返回的是method类型driver.context 窗口句柄12345# 返回当前会话中所有窗口的句柄。driver.window_handles# 返回当前窗口的句柄driver.current_window_handle 窗口切换12345678910# 通过window_handles来遍历for handle in driver.window_handles: driver.switch_to_window(handle)# 切换窗口driver.switch_to.window("窗口名")# 或通过window_handles来遍历for handle in driver.window_handles: driver.switch_to_window(handle) 历史和选项卡管理12345# 前进driver.forward()# 回退driver.back() 选项卡管理： 12345678910# 执行JavaScript脚本browser.execute_script('window.open()')print(browser.window_handles)# 切换到第二个选项卡browser.switch_to_window(browser.window_handles[1])browser.get('https://www.keymou.wang')time.sleep(2)# 再切换到第一个选项卡browser.switch_to_window(browser.window_handles[0])browser.get('https://python.org/') 定位元素Selenium 提供了以下方法定位页面中的元素： 123456789101112131415161718192021222324# 根据id 定位元素，查找 id="SL_balloon_obj"的元素，返回id属性值与位置匹配的第一个元素driver.find_element_by_id('SL_balloon_obj')# 根据name 定位元素driver.find_element_by_name('viewport')# 根据xpath 定位元素，XPath是用于在XML文档中定位节点的语言。由于HTML可以是XML（XHTML）的实现，因此Selenium用户可以利用这种强大的语言来定位其Web应用程序中的元素。driver.find_element_by_xpath('/html/head/meta[3]')driver.find_element_by_xpath("//form[1]")# 根据链接的文本来定位driver.find_element_by_link_text('Download')# 根据元素标签对之间的部分文本信息来定位driver.find_element_by_partial_link_text('帮助')# 通过tag定位driver.find_element_by_tag_name('li')# 根据class定位driver.find_element_by_class_name('FRAME_login')# 根据css定位driver.find_element_by_css_selector('head &gt; meta:nth-child(5)') 要查找多个元素，有以下方法（将 element 改为 elements），不同是这些方法返回一个列表list 类型，可通过索引的方式引用： 12345# 根据id 查找所有id="SL_balloon_obj"的元素obj = driver.find_elements_by_id('SL_balloon_obj')# 指定引用第二个 id="SL_balloon_obj"的元素并清空元素obj[1].clear() 除了上述给出的公共方法之外，还有两个私有方法可能对页面对象中的定位有用。这是两个私有方法： 123456from selenium.webdriver.common.by import By# find_element()方法driver.find_element(By.ID, 'SL_balloon_obj')# find_elements()方法driver.find_elements(By.NAME, 'WB_miniblog') 这些是 By类可用的属性： 12345678ID = "id"XPATH = "xpath"LINK_TEXT = "link text"PARTIAL_LINK_TEXT = "partial link text"NAME = "name"TAG_NAME = "tag name"CLASS_NAME = "class name"CSS_SELECTOR = "css selector" Selenium 定位到结点位置会返回一个WebElement 类型的对象，可以调用下述方法来提取需要的信息。 1234567891011121314element = driver.find_element_by_id('downloads')# 获取属性class的值element.get_attribute('class')# 获取文本element.text# 获取标签名称element.tag_name# 获取结点idelement.id# 获取位置element.location# 获取大小element.size 刷新1driver.refresh() 页面交互123456789101112from selenium.webdriver.common.keys import Keys# clear()用于清除文本输入框中的内容driver.find_element_by_id("idinput").clear()# send_keys()用于模拟键盘向输入框里输入内容driver.find_element_by_id("input").send_keys("username")# send_keys(Keys.CONTROL, 'a')模拟键盘Ctrl+A操作driver.find_element_by_id('q').send_keys(Keys.CONTROL, 'a')# click()用于进行点击操作driver.find_element_by_id("loginbtn").click() 填写表格，提交表单1234567# 在页面上找到第一个“SELECT”元素element = driver.find_element_by_xpath("//select[@name='name']")# 依次遍历每个OPTION，打印出它们的值，然后依次选择每个OPTION。all_options = element.find_elements_by_tag_name("option")for option in all_options: print("Value is: %s" % option.get_attribute("value")) option.click() 这不是处理SELECT 元素的最有效方法，WebDriver 的支持类包括一个名为 Select 的支持类，它提供了与这些类交互的有用方法： 1234567891011121314151617from selenium.webdriver.support.ui import Selectselect = Select(driver.find_element_by_name('name'))select.select_by_index(index)select.select_by_value(value)select.select_by_visible_text("text")# 取消选择选定选项select.deselect_by_index()# 取消选择所有选定选项select.deselect_all()# 所有默认选定选项的列表select = Select(driver.find_element_by_xpath("//select[@name='name']"))all_selected_options = select.all_selected_options# 获得所有可用选项options = select.options 填写完表单后，您可能想要提交表单。一种方法是找到“提交”按钮并单击它： 123# 提交表单# Assume the button has the ID "submit" :)driver.find_element_by_id("submit").click() 另外一种方法，WebDriver在每个元素上都有“提交”的便捷方法。如果在表单中的元素上调用它，WebDriver 将向上走DOM，直到找到封闭的表单，然后调用submit。但如果元素不在表单中，则会引发异常NoSuchElementException： 12# 通过定位搜索框并通过submit()提交搜索框的内容，达到点击搜索按钮的效果driver.find_element_by_id("query").submit() 模拟键盘输入send_keys()方法虽然可以模拟键盘输入，但除此之外，还需要输入其它键，比如空格，这个时候就用到 Keys(),对应类：selenium.webdriver.common.keys.Keys，使用方法：send_keys(*keys_to_send) 。类似的方法还有 send_keys_to_element(element, *keys_to_send) BACK_SPACE：删除键（BackSpace）(send_key(Keys.BACK_SPACE)) SPACE：空格键（Space） TAB：制表键（TAB） ESCAPE：回退键（Esc） ENTER：回车键（Enter） CONTROL，’a’：全选（Ctrl+A）(send_key(Keys.CONTROL, &#39;a&#39;)) CONTROL，’c’：复制（Ctrl+C） CONTROL，’x’：剪切（Ctrl+X） CONTROL，’v’：粘贴（Ctrl+V） ARROW_DOWN：向下箭头 ARROW_LEFT： 向左箭头 F1：键盘F1 … F12：键盘F12 动作链（Action Chains）ActionChains 是一种自动执行低级别交互的方法，例如鼠标移动，鼠标按钮操作，按键和上下文菜单交互。这对于执行更复杂的操作非常有用，例如悬停和拖放。生成用户操作，在ActionChains 对象上调用操作方法时，操作将存储在ActionChains 对象的队列中。当您调用perform()时，事件将按它们排队的顺序触发。 ActionChains 可用于链式模式： 1234567from selenium.webdriver import ActionChainsmenu = driver.find_element_by_css_selector(".nav")hidden_submenu = driver.find_element_by_css_selector(".nav #submenu1")actions = ActionChains(driver)# 模拟先移动鼠标到menu元素，并点击hidden_submenu元素actions.move_to_element(menu).click(hidden_submenu).perform() 或者可以逐个排队，然后执行： 1234567menu = driver.find_element_by_css_selector(".nav")hidden_submenu = driver.find_element_by_css_selector(".nav #submenu1")actions = ActionChains(driver)actions.move_to_element(menu)actions.click(hidden_submenu)actions.perform() ActionChains 可以模拟鼠标动作，提供很多方法，如单击、双击、拖拽等。 1234567891011121314151617181920212223242526272829303132333435363738394041# 导入ActionChains类from selenium.webdriver import ActionChainsactions = ActionChains(driver)# 单击某个节点，若click()传入None，则点击鼠标当前位置actions.click(element)# 单击某个节点并按住鼠标左键不放actions.clcik_and_hold(element)# 右键单击某个节点actions.context_click(element)# 双击某个节点actions.double_click(element)source = driver.find_element_by_css_selector('#droppable')target = driver.find_element_by_css_selector('#draggable')# 在source节点按住鼠标左键，移动鼠标到target节点并松开鼠标左键actions.drag_and_drop(source, target)# 在source节点按住鼠标左键，按照偏移量xoffset,yoffset移动鼠标后，松开鼠标左键actions.drag_and_drop_by_offset(source, xoffset, yoffset)# key_down()按下特殊键 (Control, Alt and Shift)，key_up()释放特殊键actions.key_down(Keys.CONTROL).send_keys('c').key_up(Keys.CONTROL).perform()# 按照偏移量移动鼠标actions.move_by_offset(xoffset, yoffset)# 鼠标移动到某个节点的位置actions.move_to_element(element)# 将鼠标移动指定元素的偏移量，偏移量相对于元素的左上角actions.move_to_element_with_offset(to_element, xoffset, yoffset)# 在几秒钟内暂停指定持续时间内的所有输入actions.pause(seconds)# 执行所有存储的操作，调用perform()才会执行actions.perform()# 释放鼠标按钮actions.release()# 清除已存储在本地和远程端的操作actions.reset_actions()# 将键发送到当前聚焦元素actions.send_keys()# 将键发送到元素actions.send_keys_to_element(element, Keys.NUMPAD8) 触摸动作（Touch Actions）Touch Actions 与ActionChains 类似，模拟用户触摸动作： 12345678910111213141516171819202122232425from selenium.webdriver.common.touch_actions import TouchActionstouch = TouchActions(driver)# 点击给定元素touch.tap()# 在给定坐标处向下触摸touch.tap_and_hold()# 在某一节点上双击touch.double_tap(on_element)# 轻弹，从屏幕上的任何地方开始touch.flick(xspeed, yspeed)# 从on_element开始轻弹，然后以指定的速度移动xoffset和yoffsettouch.flick_element(on_element, xoffset, yoffset, speed)# 长按on_elementtouch.long_press(on_element)# 将保持点击移动到指定位置touch.move(xcoord, ycoord)# 执行所有存储的操作touch.release()# 触摸并滚动，按xoffset和yoffset移动touch.scroll(xoffset, yoffset)# 触摸并滚动从on_element开始，按xoffset和yoffset移动touch.scroll_from_element(on_element, xoffset, yoffset)touch.perform()# 在指定位置释放先前发出的tap和hold命令 页面等待目前，大多数Web 应用程序都在使用AJAX 技术。当浏览器加载页面时，该页面中的元素可能以不同的时间间隔加载。这使定位元素变得困难：如果DOM 中尚未存在元素，则将引发ElementNotVisibleException 异常。 为了避免这种元素定位困难而且会提高产生ElementNotVisibleException 的概率。Selenium Webdriver 提供两种类型的等待方式：隐式等待、显式等待。 隐式等待 隐式等待比较简单，WebDriver 在尝试查找不能立即可用的任何元素时轮询DOM 一段时间。默认设置为0，设置后，将为WebDriver 对象的生命周期设置隐式等待。 1234567from selenium import webdriverdriver = webdriver.Firefox()# 隐式等待 10sdriver.implicitly_wait(10)driver.get("http://somedomain/url_that_delays_loading")myDynamicElement = driver.find_element_by_id("myDynamicElement") 显示等待 显示等待是定义的代码，用于在进一步执行代码之前等待某个条件发生。最极端情况是 time.sleep()，将等待时间设置为等待的确切时间。WebDriverWait 与 ExpectedCondition 相结合是一种可实现的方法。 1234567891011121314from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Firefox()driver.get("http://somedomain/url_that_delays_loading")try: # 抛出TimeoutException之前等待最多10秒，除非它发现元素在10秒内返回 element = WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.ID, "myDynamicElement")) )finally: driver.quit() 默认情况下，WebDriverWait 每500 毫秒调用一次 ExpectedCondition，直到成功返回。 expected_conditions 模块包含一组用于WebDriverWait 的预定义条件，具体描述如下： | 预期条件 | 描述 || :————————————: | :———————————————– || title_is | 标题是某内容 || title_contains | 标题包含某内容 || presence_of_element_located | 节点加载出来，传人定位元组如(By.ID, ‘q’) || visibility_of_element_located | 节点可见，传入定位元组 || visibility_of | 可见，传入节点对象 || presence_of_all_elements_located | 所有结点加载出来 || text_to_be_present_in_element | 某个节点文本包含某文字 || text_to_be_present_in_element_value | 某个节点值包含某文字 || frame_to_be_available_and_switch_to_it | 加载并切换 || invisibility_of_element_located | 结点不可见 || element_to_be_clickable | 节点可点击 || staleness_of | 判断一个节点是否仍在DOM ，可判断页面是否已经刷新 || element_to_be_selected | 节点可选择，传节点对象 || element_located_to_be_selected | 结点可选择，传入定位元组 || element_selection_state_to_be | 传入结点对象和状态，相等返回True，否则返回False || element_located_selection_state_to_be | 传入定位元组和状态，相等返回True，否则返回False || alert_is_present | 是否出现警告 | 感受一下： 123456from selenium.webdriver.support import expected_conditions as ECwait = WebDriverWait(driver, 10)# element_to_be_clickable()方法传入定位元组element = wait.until(EC.element_to_be_clickable((By.ID, 'someid')))wait.until(EC.presence_of_element_located((By.ID, 'content_left'))) 如果以上的都不符合，还可以创建自定义等待条件，可以使用带有__call__方法的类创建自定义等待条件，该方法在条件不匹配时返回 False。 1234567891011121314151617181920class element_has_css_class(object): """期望检查元素是否具有特定的css类 locator - 用于定位元素 只要有特定的css类，返回WebEmlement """ def __init__(self, locator, css_class): self.locator = locator self.css_class = css_class def __call__(self, driver): element = driver.find_element(*self.locator) # Finding the referenced element if self.css_class in element.get_attribute("class"): return element else: return False# Wait until an element with id='myNewInput' has class 'myCSSClass'wait = WebDriverWait(driver, 10)element = wait.until(element_has_css_class((By.ID, 'myNewInput'), "myCSSClass")) 休眠123import time# 休眠5秒time.sleep(5) iframe切换在web 应用中经常会遇到frame/iframe 表单嵌套页面的应用，webdriver 只能在一个页面上对元素识别与定位，对于frame/iframe无法直接定位； 这时候就需要通过switch_to.frame() 方法将当前定位的主体切换为frame/iframe 内嵌的的页面中： 1234567891011121314151617browser = webdriver.Firefox()browser.get('http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable')# 切换到id="iframeResul"的iframebrowser.switch_to.frame('iframeResult')try: logo = browser.find_element_by_class_name('logo')except NoSuchElementException: print('No Logo')# 切换到当前framebrowser.switch_to.parent_frame()# 切换到默认内容browser.switch_to.default_content()# iframe没有id或name情况，先通过find_element方法定位到iframe，然后传给switch_to.frame()方法driver.switch_to.frame(BROWSER.find_element_by_tag_name('iframe')) 弹窗示警Selenium WebDriver内置支持处理弹出对话框。在您触发将打开弹出窗口的操作后，您可以使用以下命令访问警报： 1234567891011121314# 将返回当前打开的警报对象alert = driver.switch_to_alert()from selenium.webdriver.common.alert import Alertalert = Alert(driver)# 接受可用的警报,确认警告对话框alert.accept()# 驳回可用的警报alert.dismiss()# 将键发送到警报alert.send_keys()# 获取alert文本内容alert.text() 上传下载文件12345678910# 通过send_keys()方法模拟上传文件driver.find_element_by_name("file").send_keys("unittest.txt")# 下载文件options = webdriver.ChromeOptions()prefs = &#123;'profile.default_content_settings.popups': 0, 'download.default_directory': os.getcwd()&#125;options.add_experimental_option('prefs', prefs)driver = webdriver.Chrome(chrome_options=options)driver.get("http://pypi.Python.org/pypi/selenium")driver.find_element_by_partial_link_text("selenium-3.11.0-py2.py3-none-any").click() 页面截图123456789# 将当前窗口的屏幕截图保存到PNG图像文件，可以传入相对路径或绝对路径driver.save_screenshot('/截图/foo.png')# 截取当前窗口，并指定截图图片的保存位置dirver.get_screenshot_as_file("screen.jpg")# 获取当前窗口的屏幕截图作为base64编码的字符串，适用于在HTML中的嵌入图像driver.get_screenshot_as_base64()# 截屏并保存为png格式图片driver.get_screenshot_as_png() cookie有些站点需要登录后才能访问，用Selenium 模拟登录后获取Cookie，然后供爬虫使用的场景非常常见，Selenium提供了获取，增加，删除Cookies 的函数。 123456789101112131415161718# 获取所有Cookiesbrowser.get_cookies()# 获取name对应的cookie信息browser.get_cookie(name)# 增加Cookies，是字典对象，必须要有name和valuebrowers.add_cookie(&#123;xxx&#125;)driver.add_cookie(&#123;'name': 'name', 'domain': 'www.zhihu.com', 'value': 'keymou'&#125;)# 如果需要遍历，则如下：for cookie in driver.get_cookies(): print("%s -&gt; %s " % (cookie["name"],cookie["value"]))# 删除所有Cookiesbrowser.delete_all_cookies()# 删除Cookie信息，name是要删除的cookie的名称，optionsString是该cookie的选项，目前支持的选项包括“路径”和“域”browser.delete_cookie(name, optionsString) 实践一下： 12345678910111213141516171819202122232425262728293031from selenium import webdriverbrowser = webdriver.Firefox()browser.get('https://www.zhihu.com/explore')# 获取浏览器cookiescookies = browser.get_cookies()# 打印cookies类型并打印cookiesprint(type(cookies))print(cookies)# 构造cookies# cookies字典包含name、value、path、domain、secure、httpOnly、expiry。# name：cookie的名称；# value：cookie对应生成的动态值；# path：定义了Web服务器上哪些路径下的页面可获取服务器设置的Cookie；# domain：服务器域名；# secure：Cookie中标记该变量；当设置为true时，表示创建的Cookie会被以安全的形式向服务器传输# httpOnly：防脚本攻击；设置为true时，则通过程序(JS脚本、Applet等)将无法读取到Cookie信息，这样能有效的防止XSS攻击。# expiry：Cookie有效终止日期。cookies = &#123; 'domain': 'www.zhihu.com', 'name': 'name', 'value': 'keymou'&#125;# add_cookie方法接受一个字典browser.add_cookie(cookies)# 打印添加过cookie后的cookiesprint(browser.get_cookies())# delete_all_cookies()删除cookiesbrowser.delete_all_cookies()# 打印cookiesprint(browser.get_cookies()) 执行代码，cookies 是个嵌套字典的列表。 在实践中我们可以通过selenium 模拟登录操作，然后通过对象的方法获取当前访问网站的session、cookie，得到cookie 之后，就可以通过urllib2 或request 访问相应的网站，并可实现网页爬取等工作。 123456789101112131415161718# 获取浏览器cookiescookies = browser.get_cookies()# 构造cookiecookie = [item['name'] + "=" + item['value'] for item in cookies]cookies = ';'.join(item for item in cookie)# 使用urllibfrom urllib import requesturl = browser.current_urlheader = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Fir', 'Cookie': cookies,&#125;res = request.Request(url=url, headers=header)response = request.urlopen(res)print(response.read().decode('utf-8')) 执行JS 语句webdriver 提供浏览器的前进和后退，但是并不提供滚动浏览器的操作，因此来借助JS 来控制浏览器的滚动条。 12345678# 使用execute_script() 方法browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert("To bottom")')# 向文本框输入文本信息text = "Hello world"js = "var sum=document.getElementById("id"); sum.value='"+text+ " ';"driver.excute_script(js) Headless在介绍Headless之前，必须介绍下PhantomJS，PhantomJS是没有界面的浏览器，特点：会把网站加载到内存并执行页面上的JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。 使用也简单，不过首先得下载 phantomjs.exe，拷贝到Python\Scripts 目录下，示例如下： 123456from selenium.webdriver import PhantomJSdriver = PhantomJS()driver.get('https://www.baidu.com')print(driver.current_url)driver.quit() 不过18年4月份维护者宣布退出PhantomJs，意味着这个项目不再维护了。 下面简单介绍下 Chrome和Firefox 浏览器的Headless 模式，Windows Chrome需要60以上的版本才支持 Headless模式，linux，unix系统需要 chrome浏览器 &gt;= 59。 123456789101112131415161718192021222324252627'''Chrome Headless模式'''import osfrom selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.chrome.options import Optionsimport timechrome_options = Options()chrome_options.add_argument("--headless")base_url = "http://www.baidu.com/"#对应的chromedriver的放置目录driver = webdriver.Chrome(executable_path=(r'C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe'), chrome_options=chrome_options)driver.get(base_url + "/")start_time=time.time()print('this is start_time ',start_time)driver.find_element_by_id("kw").send_keys("selenium webdriver")driver.find_element_by_id("su").click()driver.save_screenshot('pic\screen.png')driver.close()end_time=time.time()print('this is end_time ',end_time) 12345678910'''Firefox Headless模式'''from selenium import webdriveroptions = webdriver.FirefoxOptions()options.add_argument('-headless')browser = webdriver.Firefox(options=options)base_url = 'http://www.weather.com.cn/'browser.get(base_url) print(browser.current_url)browser.quit() Webdriver.chromechrome.options12345678910from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()# 向列表加入参数chrome_options.add_argument("--headless")chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36"')browser = webdriver.Chrome(chrome_options=chrome_options)browser.get('https://www.baidu.com')browser.quit() add_argument()方法可以添加启动参数，添加完毕后可以在初始化Webdriver 对象时将此Options 对象传入，则可以实现以特定参数启动Chrome。 常用的启动参数： 启动参数 描述 –user-agent=”” 设置请求头的User-Agent –window-size=1366,768 设置浏览器分辨率 –headless 无界面运行 –start-maximized 最大化运行 –incognito 隐身模式 –disable-javascript 禁用javascript –disable-infobars 禁用浏览器正在被自动化程序控制的提示 完整启动参数可以点此链接查看： 123456789101112131415# 禁用图片加载prefs = &#123; 'profile.default_content_setting_values' : &#123; 'images' : 2 &#125;&#125;options.add_experimental_option('prefs',prefs)# 禁用浏览器弹框prefs = &#123; 'profile.default_content_setting_values' : &#123; 'notifications' : 2 &#125; &#125; options.add_experimental_option('prefs',prefs) options 还提供其他方法： 12345678910111213141516171819202122# 将其用于提取到ChromeDriver具有扩展数据的Base64编码字符串添加到列表中chrome_options.add_encoded_extension(extension)# 添加传送给Chrome的实验选项，name为实验选项名称，value为其值chrome_options.add_experimental_option(name, value)# 添加扩展路径，extension: *.crx扩展文件的路径chrome_options.add_extension(extension)# 返回arguments参数列表chrome_options.arguments# 返回二进制文件的位置chrome_options.binary_location# 返回远程devtools实例的地址chrome_options.debugger_address# 返回chrome的实验选项字典chrome_options.experimental_options# 返回将加载到chrome中的已编码扩展名列表chrome_options.extensions# 返回是否设置headless,设置headless返回Truechrome_options.headless# 设置capalilitychrome_options.set_capability(name, value)# 设置headlesschrome_options.set_headless(headless=True) chrome.webdriver1234567891011121314151617181920212223242526from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsoptions = Options()# 向列表加入参数options.add_argument('--headless')# Chrome()传入可传入以下参数：# executable_path: chromedriver.exe的路径，方便部署项目# port：端口号# options：需要传入ChromeOptions实例，同chrome_options# desired_capabilities：仅具有非浏览器特定功能的Dictionary对象，如“proxy”或“loggingPref”。# service_args：服务器参数# service_log_path：服务器日志路径# keep_alive：是否保持长连接browser = webdriver.Chrome(executable_path='chromedriver.exe', port=0, options=options, service_log_path='\log', keep_alive=true)browser = webdriver.Chrome(chrome_options=options)# 日志类型browser.log_types()browser.create_options()# 执行Chrome Devtools Protocol命令并获取返回的结果browser.execute_cdp_cmd(cmd, cmd_args)# 获取Chrome网络仿真设置，返回类型为字典dictbrowser.get_network_conditions()# 启动ID指定的Chrome应用browser.launch_app(id) 本文完。]]></content>
      <categories>
        <category>selenium自动化</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
        <tag>自动化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫之模拟Ajax请求抓取新浪微博]]></title>
    <url>%2FPython%20%E7%88%AC%E8%99%AB%E4%B9%8B%E6%A8%A1%E6%8B%9FAjax%E8%AF%B7%E6%B1%82%E6%8A%93%E5%8F%96%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A.html</url>
    <content type="text"><![CDATA[Python 模拟Ajax 请求，抓取新浪微博。 Ajax是什么 全称为 Asynchronous JavaScript and XML，即异步的 JavaScript 和 XML，利用 JavaScript 在保证页面不被刷新、页面链接不变的情况下与服务器交换数据并更新部分网页的技术。 拿新浪微博为例，打开我的微博链接 https://m.weibo.cn/u/1715175570 ，一直下滑，可以发现下滑几个微博之后，会出现一个加载的动画，不一会儿就继续出现了新的微博内容，这个过程其实就是 Ajax 加载的过程。 Ajax 基本原理 这里不多做介绍，详细可参考 W3school 关于 Ajax 教程 分析请求 用Firefox浏览器打开我的微博链接 https://m.weibo.cn/u/1715175570 ，打开火狐浏览器自带的 Web 开发者工具，切换“网络”选项卡，可以发现这里出现很多条请求。Ajax 的请求类型是 xhr，可以通过工具栏上不同请求类型如 HTML、CSS、JS、XHR 等过滤请求。 点击”XHR“ 类型，过滤出 Ajax 请求再做分析。点击其中一条，可以查询该条请求的详细。在“消息头”选项卡中查看 请求网址，请求方法，以及请求头的详细，X-Requested-With 是 XMLHttpRequest，这就标记了此请求是 Ajax 请求。 随后可点击下 ”响应“，可以看到 JSON 格式的响应内容，返回的是个人信息，如昵称、简介等，这也是用来渲染个人主页所用的数据。 下滑页面以加载新的微博内容，可以看到，会有不断的 Ajax 请求发出。选择其中一个请求，分析它的参数信息。 这是一个 GET 请求，请求链接是 https://m.weibo.cn/api/container/getIndex?type=uid&amp;value=1715175570&amp;containerid=1076031715175570&amp;page=2 。 请求的参数有 4 个：type、value、containerid、page。 继续看接下来的请求，可以发现，type、value、containerid 始终没有变化，改变的值只有 page，很明显这个参数用来控制分页的，下图所示。 分析响应 观察这个请求的响应内容，所图所示： 响应内容是 JSON 格式的，data 数据分为两部分：cardlilstInfo、cards。cards 是一个列表，包含要提取的微博信息，比较重要的字段是 mblog。 包含的是微博一些信息，如 created_at（发布日期）、reposts_count（转发数目）、comments_count（评论数目）、attitudes_count（点赞数目）、text（微博正文）等。 这样可以请求一个接口，就可以获得一个 page 的微博，只需改变参数 page 即可。 实现代码 可以先定义一个获取单个 page 的函数 12345678910111213141516171819202122232425from urllib.parse import urlencodeimport requestsheaders = &#123; 'Host': 'm.weibo.cn', 'Referer': 'https://m.weibo.cn/u/1715175570', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0;) Gecko/20100101 Firefox/63.0', 'X-Requested-With': 'XMLHttpRequest' &#125;def get_page(page): """获取页面page微博列表""" params = &#123; 'type': 'uid', 'value': '1715175570', 'containerid': '1076031715175570', 'page': page &#125; url = base_url + urlencode(params) try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() except requests.ConnectionError as e: print('Error', e.args) 这里定义 base_url 来表示请求的 URL 前半部分，接下来，构造参数字典，调用 urlencode() 方法将参数转换成 URL 的 GET 请求参数。随后，base_url 与参数拼接成完整的 URL 。 用 requests 请求这个链接，加入 headers 参数，然后判断响应的状态码，若请求成功，返回 200，则调用 json() 方法将内容解析为 JSON 返回，否则不返回任何信息。 之后定义一个解析方法 parse_page(json)，用来从返回的 JSON 中提取信息，遍历 cards，获取 mblog 中的各个信息，赋值为一个信息的字典返回即可。 12345678910111213141516171819202122from pyquery import PyQuery as pqdef parse_page(json): """解析网页""" if json: items = json.get('data').get('cards') for item in items: item = item.get('mblog') weibo = &#123;&#125; # 发布日期 weibo['date'] = item.get('created_at') weibo['id'] = item.get('id') # 微博正文 weibo['text'] = pq(item.get('text')).text() weibo['source'] = item.get('source') # 转发数 weibo['reposts'] = item.get('reposts_count') # 评论数 weibo['comments'] = item.get('comments_count') # 点赞数 weibo['attitudes'] = item.get('attitudes_count') yield weibo 然后再定义一个方法将解析出来的结果储存到 MongoDB 中。 123456from pymongo import MongoClientdef save_to_mongo(result): """将返回结果result保存到MongoDB""" if collection.insert_many(result): print('Saved to mongodb') 另外定义一个获取长微博全文内容的方法，代码如下： 12345678910def longtext(id): """获取长微博内容""" url = 'https://m.weibo.cn/statuses/extend?id=' + id try: response = requests.get(url, headers=headers) if response.status_code == 200: longtext = response.json().get('data').get('longTextContent') return pq(longtext).text() except requests.ConnectionError as e: print('Error', e.args) 需要传入一个 id 参数，配合改写下解析方法parse_page(json)，整合代码，完整代码如下： 123456789101112131415161718192021222324from pyquery import PyQuery as pqimport reimport requestsdef parse_page(json): """解析网页""" if json: items = json.get('data').get('cards') for item in items: try: item = item.get('mblog') weibo = &#123;&#125; # 创建日期 weibo['date'] = item.get('created_at') weibo['id'] = item.get('id') # 判断是否为长微博 if item.get('isLongText') == True: content = pq(item.get('text')) result = re.search(r'&lt;a.*?status.*?(\d&#123;16&#125;).*?"', str(content)) lt_id = result.group(1) weibo['text'] = longtext(lt_id) else: # 微博正文 weibo['text'] = pq(item.get('text')).text() 修改下 get_page() 方法，将 value、containerid 同样做了参数化处理，这样可以方便爬取其他微博用户的信息。值得注意的是，返回的 JSON 内容在遍历 cards时可能会报错，原因在于微博有时会在 cards 列表中返回博主关注的信息，导致在解析时报 AttributeError，故在parse_page() 方法加上异常处理、以及抓取了转发微博的原微博内容。 最后在 main() 中通过 while 循环实现遍历所有页的微博内容。 运行结果： Studio 3T客户端展示结果如下： 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106'''获取微博'''from urllib.parse import urlencodefrom pyquery import PyQuery as pqfrom pymongo import MongoClientimport reimport timeimport randomimport requestsdef get_page(value, containerid, page): """获取页面微博列表""" params = &#123; 'type': 'uid', 'value': value, 'containerid': containerid, 'page': page &#125; url = base_url + urlencode(params) try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() except requests.ConnectionError as e: print('Error', e.args)def parse_page(json): """解析网页""" if json: items = json.get('data').get('cards') for item in items: try: item = item.get('mblog') weibo = &#123;&#125; # 创建日期 weibo['date'] = item.get('created_at') weibo['id'] = item.get('id') # 判断是否为长微博 if item.get('isLongText') == True: content = pq(item.get('text')) result = re.search(r'&lt;a.*?status.*?(\d&#123;16&#125;).*?"', str(content)) lt_id = result.group(1) weibo['text'] = longtext(lt_id) else: # 微博正文 weibo['text'] = pq(item.get('text')).text() weibo['source'] = item.get('source') # 转发数 weibo['reposts'] = item.get('reposts_count') # 评论数 weibo['comments'] = item.get('comments_count') # 点赞数 weibo['attitudes'] = item.get('attitudes_count') # 转发原文内容 if item.get('retweeted_status'): weibo['repost_text'] = pq(item.get('retweeted_status').get('text')).text() except AttributeError as e: continue yield weibodef longtext(id): """获取长微博内容""" url = 'https://m.weibo.cn/statuses/extend?id=' + id try: response = requests.get(url, headers=headers) if response.status_code == 200: longtext = response.json().get('data').get('longTextContent') # 通过pyquery方法去掉一些html标签 return pq(longtext).text() except requests.ConnectionError as e: print('Error', e.args)def save_to_mongo(result): """将返回结果result保存到MongoDB""" if collection.insert_many(result): print('Saved to mongodb')if __name__ == '__main__': value = '1862855661' containerid = '1076031862855661' base_url = 'https://m.weibo.cn/api/container/getIndex?' headers = &#123; 'Host': 'm.weibo.cn', 'Referer': 'https://m.weibo.cn/u/' + value, 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0;) Gecko/20100101 Firefox/63.0', 'X-Requested-With': 'XMLHttpRequest' &#125; myclient = MongoClient("mongodb://localhost:27017/") mydb = myclient["test"] collection = mydb["weibo" + value] page = 1 while True: print('*'*50) print('正在爬取：第%s 页' %page) json = get_page(value, containerid, page) if not json.get('ok') == 0: results = parse_page(json) save_to_mongo(results) page += 1 time.sleep(random.randint(1,4)) else: print("下载完最后一页!") break 本文完。]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
        <tag>MongoDB</tag>
        <tag>Ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫之利用requests库抓取猫眼电影排行]]></title>
    <url>%2FPython%20%E7%88%AC%E8%99%AB%E4%B9%8B%E5%88%A9%E7%94%A8requests%E5%BA%93%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C.html</url>
    <content type="text"><![CDATA[通过requests库，正则表达式抓取分析猫眼榜单100 并保存到MongoDB。 【摘要】：最近在学习爬虫，阅读崔大大的《Python 3网络爬虫开发实战》一书，获益匪浅。第一个爬虫实战就是抓取猫眼电影榜单100。 这里简单介绍下如何分析源代码，通过正则表达式爬取数据并保存到数据库。 准备工作确保已安装好 request 库，pymongo库，配置好 MongoDB 。 抓取分析抓取的目标站点是 https://maoyan.com/board/4 ，排名第一的电影是霸王别姬，页面中显示的有效信息有影片名称、主演、上映时间、上映地区、评分、海报等信息。 翻页到第二页，显示的结果是排行11~20 的电影，URL 变成 https://maoyan.com/board/4?offset=10 ，URL比之前多了一个offset 的参数，继续翻页，显示的结果是排行21~30 的电影，URL 变为 https://maoyan.com/board/4?offset=20 总结规律：URL 中参数 offset 代表偏移量值，如果偏移量为n ，则显示的电影序号就是n+1 到n+10, 每页显示10 个。获取 TOP100 电影，则分开请求10 次即可。 抓取单页面首先抓取单个页面信息，定义一个 get_one_page()方法，具体代码如下 1234567891011121314151617181920212223242526272829'''抓取猫眼电影榜单'''import reimport requestsfrom fake_useragent import UserAgentdef get_one_page(url): """获取url的电影信息""" try: ua = UserAgent() headers = &#123; # 引用fake_useragent随机生成一个User-Agent 'User-Agent': ua.random &#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text return None except RequestException: return None def main(): """ """ url = 'https://maoyan.com/board/4' html = get_one_page(url) print(html)main() 正则提取电影信息要获取页面影片名称、主演、上映时间、上映地区、评分、海报等信息，分析页面源码，利用Web 开发者工具，查看“网络”-“响应”，获取到response 源代码。 分析其中一个条目，可以看到，一部电影信息对应的源代码是一个dd 节点，我们用正则表达式来提取这里面的一些电影信息。首先，需要提取它的排名信息。排名位于 class 为 board-index 的 i 节点内，正则表达式写为： 1&apos;&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt; 随后提取电影的海报信息，后面有a 节点，其内部有两个img 节点。分别尝试打开两个 img 链接，第二个 img 节点data-src 属性是海报的链接。 提取第二个img 节点的data-src 属性，正则表达式改写为： 1&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot; 再往后，依次提取电影的名称、主演、发布时间、评分等内容，最终的正则表达式写为： 1&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt; 接下来通过调用 findall()方法提取出所有的内容。 实现代码如下： 1234567891011121314def parse_one_page(html): """解析单页电影信息""" pattern = re.compile(r'&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src="(.*?)".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html) # 遍历提取结果，去掉提取结果中不必要的信息（主演、上映时间）并生成字典 for item in items: yield &#123; 'index': item[0], 'image': item[1], 'title': item[2].strip(), 'actor': item[3].strip()[3:] if len(item[3]) &gt; 3 else '', 'time': item[4].strip()[5:] if len(item[4]) &gt; 5 else '', 'score': item[5].strip() + item[6].strip() &#125; 写入MongoDB数据实现代码如下： 12345678def write_to_mongodb(data): """写入MongoDB数据库""" # 创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。 myclient = pymongo.MongoClient("mongodb://localhost:27017/") # 创建数据库test和集合collections，注意使用[] mydb = myclient["test"] mycol = mydb["top100"] mycol.insert_one(data) 代码整合实现main()方法来调用前面实现的方法，将单页的电影结果写入到数据库。 123456def main(): """main()""" url = 'https://maoyan.com/board/4' html = get_one_page(url) for item in parse_one_page(html): write_to_mongodb(item) 分页爬取因为我们需要抓取的是TOP100 的电影，所以还需要遍历一下，给这个链接传入offset 参数，实现其他90 部电影的爬取，此时添加如下调用即可： 123if __name__ == '__main__': for i in range(10): main(offset=i*10) 对应的修改下main()函数，接收一个offset 值作为偏移量，然后构造URL 进行爬取。 123456def main(offset): """main()""" url = 'https://maoyan.com/board/4?offset='+ str(offset) html = get_one_page(url) for item in parse_one_page(html): write_to_mongodb(item) 运行结果电影榜单TOP100 成功保存到 MongoDB 数据库中。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263'''抓取猫眼电影排行'''import reimport timeimport pymongoimport requestsfrom requests.exceptions import RequestExceptionfrom fake_useragent import UserAgentdef get_one_page(url): """获取url的电影信息""" try: ua = UserAgent() headers = &#123; # 引用fake_useragent随机生成一个User-Agent 'User-Agent': ua.random &#125; response = requests.get(url, headers=headers) if response.status_code == 200: return response.text return None except RequestException: return Nonedef parse_one_page(html): """解析单页电影信息""" pattern = re.compile(r'&lt;dd&gt;.*?board-index-.*?&gt;(.*?)&lt;/i&gt;.*?data-src="(.*?)".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) items = re.findall(pattern, html) for item in items: yield &#123; 'index': item[0], 'image': item[1], 'title': item[2].strip(), 'actor': item[3].strip()[3:] if len(item[3]) &gt; 3 else '', 'time': item[4].strip()[5:] if len(item[4]) &gt; 5 else '', 'score': item[5].strip() + item[6].strip() &#125;def write_to_mongodb(data): """写入MongoDB数据库""" # 创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。 myclient = pymongo.MongoClient("mongodb://localhost:27017/") # 创建数据库test和集合collections，注意使用[] mydb = myclient["test"] mycol = mydb["top100"] mycol.insert_one(data)def main(offset): """main()""" url = 'https://maoyan.com/board/4?offset='+ str(offset) html = get_one_page(url) for item in parse_one_page(html): write_to_mongodb(item)if __name__ == '__main__': for i in range(10): main(offset=i*10) # 每请求一次，增加了一个延时等待，防止请求过快 time.sleep(1.5) 参考链接：《Python 3网络爬虫开发实战 ,崔庆才著》 Python Mongodb]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tesserocr解析库]]></title>
    <url>%2Ftesserocr%E8%A7%A3%E6%9E%90%E5%BA%93.html</url>
    <content type="text"><![CDATA[OCR，即Optical Character Recognition，光学字符识别。 是指通过扫描字符，然后通过其形状将其翻译成电子文本的过程。对于图形验证码来说，它们都是一些不规则的字符，这些字符确实是由字符稍加扭曲变换得到的内容。 tesserocr 是 Python 的一个OCR 识别库，但其实是对 tesseract 做的一层Python API 封装，所以它的核心是 tesseract 。在安装 tesserocr 之前，需要先安装 tesseract。 链接 tesserocr GitHub tesserocr PyPI tesseract 下载地址：DownLoad tesseract GitHub tesseract 语言包 tesseract下载地址：点这里 带有 dev 的为开发版本 带alpha 的为内部测试版本 带beta 的为公开测试版 带 rc 的为Release Candidate（候选版本） 其他为稳定版本，推荐选择 tesseract-3.05 的稳定版本。 下载完成后双击，可以勾选 Additional language data ( download）选项来安装OCR 识别支持的语言包，这样OCR便可以识别多国语言，然后一路默认，点击 Next 按钮即可。 不建议勾选Additional language data ( download）选项，因为速度比较慢，可以安装后直接下载语言包，然后将语言包复制到安装目录的 tessdata 目录下即可。 tesserocr使用命令： pip install tesserocr pillow 若安装报错，提示信息如下： 1error: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft Visual C++ Build Tools&quot;: https://visualstudio.microsoft.com/downloads/ 解决方法： 下载对应版本的whl 包（和下载的tesseract 版本对应），地址，然后使用命令行安装： 1pip install &lt;package_name&gt;.whl 验证安装以如下面所示的图片为样例进行测试。 首先使用命令行进行测试，使用 tesseract 命令： 1234C:\Users\Keymou\Desktopλ tesseract image.png result -l eng &amp;&amp; cat result.txtTesseract Open Source OCR Engine v3.05.02 with LeptonicaPython3WebSpider 其中第一个参数为图片名称，第二个参数result 为结果保存的目标文件名称，-l 指定使用的语言包，在此使用英文（ eng ）。然后，再用cat 命令将结果输出。 运行结果便是图片的识别结果：Python3WebSpider。 1tesseract imagename|stdin outputbase|stdout [options...] [configfile...] 用Python 代码来测试，借助于 tesserocr 库。 测试代码 1234import tesserocrfrom PIL import Imageimage = Image.open(r'D:\\python\\image.png')print(tesserocr.image_to_text(image)) 运行结果如下： Python3WebSpider 或者 直接在 cmd 中 直接调用 file_to_text()方法，代码过程如下： 123456λ pythonPython 3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)] :: Anaconda, Inc. on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import tesserocr&gt;&gt;&gt; print(tesserocr.file_to_text(&apos;image.png&apos;))Python3WebSpider 推荐一个颜值很高的 cmd 工具，Cmder。 如果成功输出结果，则证明 tesseract 和 tesserocr 都已经安装成功。 FAQ安装 tesserocr 折腾了很久，安装过程或许会碰到意外情况，不要慌，首先上网搜索看看，是否能解决，根据实际情况排查解决问题。 常见报错信息： RuntimeError: Failed to init API, possibly an invalid tessdata path: D:\ProgramData\Anaconda3\ 分析错误信息，是初始化API 失败，可能是一个无效的tessdata 路径。检查下后面给出path 下是否有 tessdata 目录。 第一种情况，没有该目录，则需要新建目录，然后将 tesseract 安装目录下的tessdata 复制该path 下。若提示的path 存在并且已经有tessdata 目录，就需要检查下是否环境变量没有配置。新增环境变量 TESSDATA_PREFIX，变量值为 指向 tessdata 的路径，如 C:\Tesseract-OCR\tessdata 第二种情况，path目录下存在 tessdata，环境变量也已生效，但python 代码测试时仍报 API 初始化错误，可能需要考虑是否版本问题。我折腾了很久，安装的 tesseract 4.0，但一直报错，换成tesseract-3.05，问题解决。 python3 通过Anaconda3 安装的，可以通过以下命令试安装： &gt; conda install -c simonflueckiger tesserocr]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python爬虫</tag>
        <tag>tesserocr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发生命周期]]></title>
    <url>%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F.html</url>
    <content type="text"><![CDATA[计算机行业流行一个笑话：有三样东西在制造过程中是永远看不见的——法律、香肠和软件。 这种说法不完全对，但有些软件开发严格有序，有些软件控制却混乱不堪。软件产品从最初构思到公开发行的过程称为软件开发生命周期模式。 以下是常用的模式： 大爆炸模式 最简单的软件开发模式，优点是简单。计划、进度安排和正规开发过程几乎没有，所有精力花费在开发软件和编写代码上。 编写边改模式 通常只有粗略的想法，进行一些简单的设计，然后开始漫长的来回编写、测试和修改缺陷的过程，反复直到觉得足够了，就发布产品。 瀑布模式 最典型的预见性的方法，严格遵循预先计划的需求分析、设计、编码、集成、测试、维护的步骤顺序进行。步骤成果作为衡量进度的方法，例如需求规格，设计文档，测试计划和代码审阅等等。 瀑布式的主要的问题是它的严格分级导致的自由度降低，项目早期即作出承诺导致对后期需求的变化难以调整，代价高昂。瀑布式方法在需求不明并且在项目进行过程中可能变化的情况下基本是不可行的。 螺旋模式 核心就在于不需要在刚开始的时候就把所有事情都定义的清清楚楚。轻松上阵，定义最重要的功能，实现它，然后听取客户的意见，之后再进入到下一个阶段。 螺旋模式每一次循环包括6个步骤： 1）明确目标、可选方案和限制条件。 2）明确并化解风险。 3）评估可选方案。 4）当前阶段的开发和测试。 5）计划下一阶段。 6）确定进入下一阶段的方法。 螺旋模型很大程度上是一种风险驱动的方法体系，因为在每个阶段之前及经常发生的循环之前，都必须首先进行风险评估。 迭代式开发 也被称作迭代增量式开发或迭代进化式开发，是一种与传统的瀑布式开发相反的软件开发过程，它弥补了传统开发方式中的一些弱点，具有更高的成功率和生产率。 每次只设计和实现这个产品的一部分，逐步逐步完成的方法叫迭代开发，每次设计和实现一个阶段叫做一个迭代。 在迭代开发方法中，每一次迭代都包括了需求分析、设计、实现与测试。采用这种方法，开发工作可以在需求被完整地确定之前启动，并在一次迭代中完成系统的一部分功能或业务逻辑的开发工作。再通过客户的反馈来细化需求，并开始新一轮的迭代。 迭代式开发的优点： 1、降低风险 2、得到早期用户反馈 3、持续的测试和集成 4、使用变更 5、提高复用性 敏捷软件开发 又称敏捷开发，是一种应对快速变化的需求的一种软件开发能力。更强调程序员团队与业务专家之间的紧密协作、面对面的沟通、频繁交付新的软件版本、紧凑而自我组织型的团队、能够很好地适应需求变化的代码编写和团队组织方法，也更注重软件开发中人的作用。 敏捷开发的目的： 通过过程和工具理解个人和交流的作用 通过全面的文档理解运行的软件 通过合同和谈判得到客户的协作 在计划的执行中做出对变更的响应 迭代开发是一种软件开发的生命周期模型，敏捷开发是多种软件开发项目管理方法的集合，其中包括了XP、Scrum等，简单来说，迭代式开发模型是敏捷开发普遍使用的软件生命周期模型，敏捷开发所包含的内容比迭代模型宽泛的多。]]></content>
      <categories>
        <category>软件开发</category>
      </categories>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[アンナチュラル]]></title>
    <url>%2F%E3%82%A2%E3%83%B3%E3%83%8A%E3%83%81%E3%83%A5%E3%83%A9%E3%83%AB.html</url>
    <content type="text"><![CDATA[又是一个周末啦，安利一部日剧《アンナチュラル》非自然死亡 简介三澄美琴（石原里美 饰）是在民间法医组织“UDI”工作的女法医，该组织专门接收由于非正常原因导致死亡的遗体，对其进行解剖以求找到案件的真相。和美琴一起工作的，还有法医中堂系（井浦新 饰）、记录员九部六郎（洼田正孝 饰）和检查技师东海林夕子（市川实日子 饰）等人。 中堂系虽然拥有着丰富的临床经验，个性却乖僻古怪，对正义和法律理解不同的美琴和中堂之间，常常产生无法调和的矛盾。其实，中堂有一个无人知晓的秘密，他的女友在一场“意外”中不幸丧生，可种种蛛丝马迹向中堂揭示了，是一名连环杀人犯取走了女友的性命。中堂不畏人言坚定的留在UDI，正是为了找到杀死女友的凶手。 剧情我就不剧透了，优酷、芒果有资源。 优酷 | 芒果TV 经典台词伴侣就要找那种睡相让你觉得很喜欢的人男女关系中是不会只有一方有错的有工夫绝望的话 还不如吃点好吃的去睡觉呢只是把孩子当作自己的所有物，不明白孩子跟自己是互相独立的个体对女性的歧视人这种生物 不管是谁切开来剥皮后都只是一团肉而已死了就明白了 特别认同中堂这段话，人生不就是这样嘛 为了活下去 梦想什么的也没必要说得那么夸张有个目标就行 每个人都是罪人 为了赎罪而工作 不管女性穿什么样的衣服或者喝得酩酊大醉都不能成为肆意妄为的理由没有得到双方一致同意的性行为就是犯罪 欺凌杀人 你就算献出了自己的生命你的痛楚肯定也无法传达给他们你的人生 &nbsp;属于你自己 看官点个赞再走吧]]></content>
      <categories>
        <category>日剧</category>
      </categories>
      <tags>
        <tag>石原里美</tag>
        <tag>日剧</tag>
        <tag>Unnatural</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常见知识点]]></title>
    <url>%2Fpython%E5%B8%B8%E8%A7%81%E7%9F%A5%E8%AF%86%E7%82%B9.html</url>
    <content type="text"><![CDATA[最近在学习python，总结了下python常用的知识点。 函数参数传递1234567891011a = 1def fun(a): a = 2fun(a)print(a)b = []def fun(b): b.append(1)fun(b)print(b) 所有的变量都可以理解是内存中一个对象的“引用”。类型是属于对象的，而不是变量。而对象有两种,“可更改”（mutable）与“不可更改”（immutable）对象。在python中，strings, tuples, 和numbers是不可更改的对象，而list,dict等则是可以修改的对象。 实例方法、类方法、静态方法12345678910111213141516171819def foo(x): print("executing foo(%s)"%(x))class A(object): def foo(self,x): print("executing foo(%s,%s)"%(self,x)) @classmethod def class_foo(cls,x): print("executing class_foo(%s,%s)"%(cls,x)) @staticmethod def static_foo(x): print("executing static_foo(%s)"%x)a=A()a.foo(2)a.class_foo(2)a.static_foo(2) 先理解下函数参数里面的self和cls。这个self和cls是对类或者实例的绑定,对于一般的函数来说我们可以这么调用foo(x),这个函数就是最常用的,它的工作跟任何东西(类,实例)无关.对于实例方法,我们知道在类里每次定义方法的时候都需要绑定这个实例,就是foo(self, x),为什么要这么做呢?因为实例方法的调用离不开实例,我们需要把实例自己传给函数,调用的时候是这样的a.foo(x)(其实是foo(a, x)).类方法一样,只不过它传递的是类而不是实例,A.class_foo(x).注意这里的self和cls可以替换别的参数,但是python的约定是这俩,还是不要改的好. 对于静态方法其实和普通的方法一样,不需要对谁进行绑定,唯一的区别是调用的时候需要使用a.static_foo(x)或者A.static_foo(x)来调用. \ 实例方法 类方法 静态方法 a = A() a.foo(x) a.class_foo(x) a.static_foo(x) A 不可用 A.class_foo(x) A.static_foo(x) 类变量、实例变量123456789class Person: name="aaa" p1=Person()p2=Person()p1.name="bbb"print(p1.name) # bbbprint(p2.name) # aaaprint(Person.name) # aaa 类变量就是供类使用的变量,实例变量就是供实例使用的。 Python自省123a = 1b = 'Hello'print(type(a), type(b)) 自省就是面向对象的语言所写的程序在运行时,所能知道对象的类型.简单一句就是运行时能够获得对象的类型。比如type()、dir()、getattr()、hasattr()、isinstance() 列表推导式123456789101112multiples = [i for i in range(30) if i % 3 is 0]print(multiples)mcase = &#123;'a': 10, 'b': 34, 'A': 7, 'Z': 3&#125;mcase_frequency = &#123; k.lower(): mcase.get(k.lower(), 0) + mcase.get(k.upper(), 0) for k in mcase.keys()&#125;print(mcase_frequency)# mcase_frequency == &#123;'a': 17, 'z': 3, 'b': 34&#125; 列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表。规范:variable = [out_exp for out_exp in input_list if out_exp == 2] 字典推导式，上述例子把同一个字母但不同大小写的值合并起来。可以快速对换一个字典的键和值：{v: k for k, v in dict.items()} 单下划线和双下划线12345678910class MyClass(): def __init__(self): self.__superprivate = 'Hello' self._semiprivate = ', world'mc = MyClass()print(mc._semiprivate)print(mc.__dict__)# print(mc.__superprivate)# AttributeError: 'MyClass' object has no attribute '__superprivate' __foo__：一种约定,Python内部的名字,用来区别其他用户自定义的命名,以防冲突. _foo：一种约定,用来指定变量私有.程序员用来指定私有变量的一种方式. __foo：这个有真正的意义:解析器用_classname__foo来代替这个名字,以区别和其他类相同的命名. 字符串格式化：%和.format的区别1234567name = 'Joe'print('Name is %s' %name)print('Name is &#123;&#125;'.format(name))name = (1, 2, 3)# print('Name is %s' %name) # TypeError: not all arguments converted during string formattingprint('Name is &#123;&#125;'.format(name))print('Name is %s' %(name,)) format简洁，%无法同时传递一个变量和元组 迭代器和生成器123456789101112131415161718192021222324252627mylist = [1, 2, 3]for i in mylist: print(i)# mylist is an iterable# These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.# yield is a keyword that is used like return, except the function will return a generator.def createGenerator(): mylist = range(3) for i in mylist: yield i*i# create a generatormygenerator = createGenerator()print(mygenerator)for i in mygenerator: print(i)print('****')# To master yield, you must understand that when you call the function, the code you have written in the function body does not run.for j in mygenerator: print(j)"""Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly.""" *args and **kwargs”不确定你的函数里将要传递多少参数时你可以用*args。例如，它可以传递任意数量的参数12345def print_everything(*args): for count, thing in enumerate(args): print('&#123;0&#125;. &#123;1&#125;'.format(count, thing))print_everything('apple', 'banana', 'cabbage') **kwargs允许你使用没有事先定义的参数名 12345def table_things(**kwargs): for name, value in kwargs.items(): print('&#123;0&#125;=&#123;1&#125;'.format(name, value))table_things(apple = 'fruit', cabbage = 'vegetable') 也可以混着用.命名参数首先获得参数值然后所有的其他参数都传递给*args和**kwargs。命名参数在列表的最前端，*args和**kwargs可以同时在函数的定义中，但是*args必须在**kwargs前面. 面向切面编程AOP和装饰器 装饰器是一个很著名的设计模式，经常被用于有切面需求的场景，较为经典的有插入日志、性能测试、事务处理等。 装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 123456789def makebold(fn): def wrapped(): return "&lt;b&gt;" + fn() + "&lt;/b&gt;" return wrappeddef makeitalic(fn): def wrapped(): return "&lt;i&gt;" + fn() + "&lt;/i&gt;" return wrapped 装饰器的作用就是为已经存在的对象添加额外的功能 123456@makebold@makeitalicdef hello(): return "hello world"print(hello()) 鸭子类型当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。并不关心对象是什么类型，到底是不是鸭子，只关心行为。在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。 Python中函数重载 函数重载主要是为了解决两个问题:可变参数类型、可变参数个数。 一个基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载。情况1：函数功能相同，但是参数类型不同，根本不需要处理，因为python可以接受任何类型的参数情况2：函数功能相同，但参数个数不同，答案就是缺省参数。python 自然就不需要函数重载。 new和init的区别 __new__是一个静态方法,而__init__是一个实例方法. __new__方法会返回一个创建的实例,而__init__什么都不返回. 只有在__new__返回一个cls的实例时后面的__init__才能被调用. 当创建一个新实例时调用__new__,初始化一个实例时用__init__. ps: metaclass是创建类时起作用.所以我们可以分别使用metaclass,new和init来分别在类创建,实例创建和实例初始化的时候做一些小手脚. 单例模式使用new方法123456789class Singleton(object): def __new__(cls, *args, **kw): if not hasattr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls, *args, **kw) return cls._instance class MyClass(Singleton): a = 1 共享属性 创建实例时把所有实例的__dict__指向同一个字典,这样它们具有相同的属性和方法.123456789class Borg(object): _state = &#123;&#125; def __new__(cls, *args, **kw): ob = super(Borg, cls).__new__(cls, *args, **kw) ob.__dict__ = cls._state return ob class MyClass2(Borg): a = 1 装饰器版本1234567891011def singleton(cls, *args, **kw): instances = &#123;&#125; def getinstance(): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return getinstance @singletonclass MyClass: ... import方法123456# mysingleton.pyclass My_Singleton(object): def foo(self): pass my_singleton = My_Singleton() 123# to use# from mysingleton import my_singleton# my_singleton.foo() 作用域 一个变量的作用域总是由在代码中被赋值的地方所决定的。 当 Python 遇到一个变量的话他会按照这样的顺序进行搜索： 本地作用域（Local）→当前作用域被嵌入的本地作用域（Enclosing locals）→全局/模块作用域（Global）→内置作用域（Built-in） 闭包(closure)闭包(closure)是函数式编程的重要的语法结构创建一个闭包必须满足以下几点: 必须有一个内嵌函数 内嵌函数必须引用外部函数中的变量 外部函数的返回值必须是内嵌函数 lambda函数lambda 表达式，通常是在需要一个函数，但是又不想费神去命名一个函数的场合下使用，也就是指匿名函数。1map( lambda x: x*x, [y for y in range(10)] ) Python里的拷贝 copy浅拷贝，没有拷贝子对象，所以原始数据改变，子对象会改变 深拷贝，包含对象里面的自对象的拷贝，所以原始对象的改变不会造成深拷贝里任何子元素的改变1234567891011121314151617181920import copya = [1, 2, 3, 4, ['a', 'b']] #原始对象 b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝 a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象 print('a = ', a)print('b = ', b)print('c = ', c)print('d = ', d)# 输出结果：# a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# c = [1, 2, 3, 4, ['a', 'b', 'c']]# d = [1, 2, 3, 4, ['a', 'b']] Python垃圾回收机制Python GC主要使用引用计数（reference counting）来跟踪和回收垃圾。在引用计数的基础上，通过“标记-清除”（mark and sweep）解决容器对象可能产生的循环引用问题，通过“分代回收”（generation collection）以空间换时间的方法提高垃圾回收效率。 引用计数PyObject是每个对象必有的内容，其中ob_refcnt就是做为引用计数。当一个对象有新的引用时，它的ob_refcnt就会增加，当引用它的对象被删除，它的ob_refcnt就会减少.引用计数为0时，该对象生命就结束了。 标记-清除机制基本思路是先按需分配，等到没有空闲内存的时候从寄存器和程序栈上的引用出发，遍历以对象为节点、以引用为边构成的图，把所有可以访问到的对象打上标记，然后清扫一遍内存空间，把所有没标记的对象释放。 分代技术整体思想是：将系统中的所有内存块根据其存活时间划分为不同的集合，每个集合就成为一个“代”，垃圾收集频率随着“代”的存活时间的增大而减小，存活时间通常利用经过几次垃圾回收来度量。 Python的isis是对比地址，==是对比值 read，readline和readlines read 读取整个文件 readline 读取下一行,使用生成器方法 readlines 读取整个文件到一个迭代器以供我们遍历]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
